{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "from polyglot.detect import Detector\n",
    "import icu\n",
    "from pprint import pprint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokenizer for tweets\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Stemmer\n",
    "snow = SnowballStemmer( 'english', ignore_stopwords=True )\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer( text ):\n",
    "    tokenized_text = []\n",
    "    \n",
    "    tokenized = tweet_tokenizer.tokenize(text)\n",
    "    for item in tokenized:\n",
    "        match = re.match('#?[A-Za-z]+', item)\n",
    "        if(match):\n",
    "            result = match.group(0)\n",
    "            url_filter = re.match('https?', result)\n",
    "            #letter_filter = re.match('[A-Za-z]', result) # any other match can be done this way\n",
    "            if( url_filter == None ):\n",
    "                real_word = re.findall('\\w+', result)[0]\n",
    "                tokenized_text.append(real_word)\n",
    "    #print( [ x for x in tokenized_text if ( len(x) > 1 ) ] )\n",
    "    return [ x for x in tokenized_text if ( len(x) > 1 ) ] # REMOVE words that are only 1 letter long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemmer( tokenized_text ):\n",
    "    return [ snow.stem(word) for word in tokenized_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_no_stopwords( tokenized_text ):\n",
    "    return [ w for w in tokenized_text if not w in stop_words ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detector(text):\n",
    "    '''Returns language type as string or 'un' if unknown.'''\n",
    "    try:\n",
    "        polyglot = Detector(text, quiet=True).languages[0]\n",
    "        if polyglot.confidence > 90: # threshold for detecting languages\n",
    "            lang = str(polyglot.locale)\n",
    "        else:\n",
    "            lang = 'un'\n",
    "    except Exception as e:\n",
    "        print(\"Error: \" + str(e))\n",
    "        lang = 'un' # if error occurs during conversion language is unknown\n",
    "    # print('Guess: ', lang)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_language_filtering( text ):\n",
    "    lang = language_detector(text)\n",
    "    if( lang == 'en' ):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @, #, numbers, and non-alphabetic characters are filtered, then the text is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'classes',\n",
       " 'with',\n",
       " 'all',\n",
       " 'due',\n",
       " 'respect',\n",
       " 'your',\n",
       " 'holiness',\n",
       " 'don',\n",
       " 'support',\n",
       " 'gmos',\n",
       " 'leptismagna',\n",
       " 'in',\n",
       " 'libya',\n",
       " 'was',\n",
       " 'the',\n",
       " 'breadbasket',\n",
       " 'for',\n",
       " 'europe',\n",
       " 'lo']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = text_tokenizer( text = \"3 class classes With f u i all due respect Your Holiness; don't support #GMOs. #LeptisMagna in #Libya was the breadbasket for #Europe. Loâ€¦ https://t.co/zwTYvJu0As\")\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language detection works better after text is filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens are rejoined, then language is checked\n",
    "English_language_filtering( ' '.join(test_tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class -> class\n",
      "classes -> class\n",
      "with -> with\n",
      "all -> all\n",
      "due -> due\n",
      "respect -> respect\n",
      "your -> your\n",
      "holiness -> holi\n",
      "don -> don\n",
      "support -> support\n",
      "gmos -> gmos\n",
      "leptismagna -> leptismagna\n",
      "in -> in\n",
      "libya -> libya\n",
      "was -> was\n",
      "the -> the\n",
      "breadbasket -> breadbasket\n",
      "for -> for\n",
      "europe -> europ\n",
      "lo -> lo\n"
     ]
    }
   ],
   "source": [
    "snow = SnowballStemmer( 'english', ignore_stopwords=True )\n",
    "for word in test_tokens:\n",
    "    print( word,'->',snow.stem(word) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abiricz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class -> class\n",
      "classes -> class\n",
      "with -> with\n",
      "all -> all\n",
      "due -> due\n",
      "respect -> respect\n",
      "your -> your\n",
      "holiness -> holiness\n",
      "don -> don\n",
      "support -> support\n",
      "gmos -> gmos\n",
      "leptismagna -> leptismagna\n",
      "in -> in\n",
      "libya -> libya\n",
      "was -> wa\n",
      "the -> the\n",
      "breadbasket -> breadbasket\n",
      "for -> for\n",
      "europe -> europe\n",
      "lo -> lo\n"
     ]
    }
   ],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "for word in test_tokens:\n",
    "    print( word,'->', lemma.lemmatize(word) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'classes',\n",
       " 'due',\n",
       " 'respect',\n",
       " 'holiness',\n",
       " 'support',\n",
       " 'gmos',\n",
       " 'leptismagna',\n",
       " 'libya',\n",
       " 'breadbasket',\n",
       " 'europe',\n",
       " 'lo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_stopwords( test_tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tweet_processing( filename_in, filename_out ):\n",
    "    print('Input:', filename_in, '\\t', 'Output:', filename_out)\n",
    "    i = 0\n",
    "    file_out = gzip.open(filename_out, mode='wb')\n",
    "    acctime = 0\n",
    "\n",
    "    with io.TextIOWrapper( io.BufferedReader(gzip.open(filename_in)) ) as file_in:\n",
    "        for line in file_in: # read one line at a time from a compressed file\n",
    "            t1 = datetime.datetime.now()\n",
    "            json_parsed = json.loads(line) # parse only that one line to json\n",
    "            tweet = json_parsed['text'] # get tweet message\n",
    "            \n",
    "            tweet_tokenized = text_tokenizer(tweet) # tokenize tweet message\n",
    "            tweet_stemmed = text_stemmer( tweet_tokenized ) # stem tokenized words\n",
    "            tweet_filtered = text_no_stopwords( tweet_stemmed ) # remove stop words\n",
    "            \n",
    "            lang_bool = English_language_filtering( ' '.join(tweet_tokenized) ) # True: English, False: not English\n",
    "            \n",
    "            # check process status\n",
    "            if i % 1000000 == 0:\n",
    "                print('status:', i, ', elapsed time:', np.round(acctime*0.001/60/60, 3), 'hour(s)', ', time to finish:', np.round(acctime*(10**8-i)/(i+1)*0.001/60/60, 1), 'hour(s)' )\n",
    "            if lang_bool == True: # if conditions are met write to file\n",
    "                file_out.write( ' '.join(tweet_filtered).encode() ) # write to file\n",
    "                file_out.write( '\\n'.encode() ) # write a newline char to the end\n",
    "            t2 = datetime.datetime.now()\n",
    "            dt = (t2-t1).microseconds*0.001\n",
    "            acctime = acctime + dt \n",
    "            i += 1\n",
    "    file_in.close()\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: World_twitterdata_part1.txt.gz \t Output: World_twitterdata_ready_part1.txt.gz\n",
      "status: 0 , elapsed time: 0.0 hour(s) , time to finish: 0.0 hour(s)\n",
      "status: 1000000 , elapsed time: 0.091 hour(s) , time to finish: 9.0 hour(s)\n",
      "status: 2000000 , elapsed time: 0.179 hour(s) , time to finish: 8.8 hour(s)\n",
      "status: 3000000 , elapsed time: 0.268 hour(s) , time to finish: 8.7 hour(s)\n",
      "status: 4000000 , elapsed time: 0.361 hour(s) , time to finish: 8.7 hour(s)\n",
      "status: 5000000 , elapsed time: 0.444 hour(s) , time to finish: 8.4 hour(s)\n",
      "status: 6000000 , elapsed time: 0.53 hour(s) , time to finish: 8.3 hour(s)\n",
      "status: 7000000 , elapsed time: 0.617 hour(s) , time to finish: 8.2 hour(s)\n",
      "status: 8000000 , elapsed time: 0.7 hour(s) , time to finish: 8.1 hour(s)\n",
      "status: 9000000 , elapsed time: 0.789 hour(s) , time to finish: 8.0 hour(s)\n",
      "status: 10000000 , elapsed time: 0.874 hour(s) , time to finish: 7.9 hour(s)\n",
      "status: 11000000 , elapsed time: 0.957 hour(s) , time to finish: 7.7 hour(s)\n",
      "status: 12000000 , elapsed time: 1.04 hour(s) , time to finish: 7.6 hour(s)\n",
      "status: 13000000 , elapsed time: 1.12 hour(s) , time to finish: 7.5 hour(s)\n",
      "status: 14000000 , elapsed time: 1.198 hour(s) , time to finish: 7.4 hour(s)\n",
      "status: 15000000 , elapsed time: 1.29 hour(s) , time to finish: 7.3 hour(s)\n",
      "status: 16000000 , elapsed time: 1.382 hour(s) , time to finish: 7.3 hour(s)\n",
      "status: 17000000 , elapsed time: 1.471 hour(s) , time to finish: 7.2 hour(s)\n",
      "status: 18000000 , elapsed time: 1.561 hour(s) , time to finish: 7.1 hour(s)\n",
      "status: 19000000 , elapsed time: 1.654 hour(s) , time to finish: 7.1 hour(s)\n",
      "status: 20000000 , elapsed time: 1.747 hour(s) , time to finish: 7.0 hour(s)\n",
      "status: 21000000 , elapsed time: 1.835 hour(s) , time to finish: 6.9 hour(s)\n",
      "status: 22000000 , elapsed time: 1.923 hour(s) , time to finish: 6.8 hour(s)\n",
      "status: 23000000 , elapsed time: 2.006 hour(s) , time to finish: 6.7 hour(s)\n",
      "status: 24000000 , elapsed time: 2.087 hour(s) , time to finish: 6.6 hour(s)\n",
      "status: 25000000 , elapsed time: 2.17 hour(s) , time to finish: 6.5 hour(s)\n",
      "status: 26000000 , elapsed time: 2.252 hour(s) , time to finish: 6.4 hour(s)\n",
      "status: 27000000 , elapsed time: 2.332 hour(s) , time to finish: 6.3 hour(s)\n",
      "status: 28000000 , elapsed time: 2.418 hour(s) , time to finish: 6.2 hour(s)\n",
      "status: 29000000 , elapsed time: 2.501 hour(s) , time to finish: 6.1 hour(s)\n",
      "status: 30000000 , elapsed time: 2.584 hour(s) , time to finish: 6.0 hour(s)\n",
      "status: 31000000 , elapsed time: 2.656 hour(s) , time to finish: 5.9 hour(s)\n",
      "status: 32000000 , elapsed time: 2.723 hour(s) , time to finish: 5.8 hour(s)\n",
      "status: 33000000 , elapsed time: 2.789 hour(s) , time to finish: 5.7 hour(s)\n",
      "status: 34000000 , elapsed time: 2.848 hour(s) , time to finish: 5.5 hour(s)\n",
      "status: 35000000 , elapsed time: 2.93 hour(s) , time to finish: 5.4 hour(s)\n",
      "status: 36000000 , elapsed time: 3.0 hour(s) , time to finish: 5.3 hour(s)\n",
      "status: 37000000 , elapsed time: 3.061 hour(s) , time to finish: 5.2 hour(s)\n",
      "status: 38000000 , elapsed time: 3.126 hour(s) , time to finish: 5.1 hour(s)\n",
      "status: 39000000 , elapsed time: 3.194 hour(s) , time to finish: 5.0 hour(s)\n",
      "status: 40000000 , elapsed time: 3.253 hour(s) , time to finish: 4.9 hour(s)\n",
      "status: 41000000 , elapsed time: 3.325 hour(s) , time to finish: 4.8 hour(s)\n",
      "status: 42000000 , elapsed time: 3.394 hour(s) , time to finish: 4.7 hour(s)\n",
      "status: 43000000 , elapsed time: 3.455 hour(s) , time to finish: 4.6 hour(s)\n",
      "status: 44000000 , elapsed time: 3.519 hour(s) , time to finish: 4.5 hour(s)\n",
      "status: 45000000 , elapsed time: 3.59 hour(s) , time to finish: 4.4 hour(s)\n",
      "status: 46000000 , elapsed time: 3.658 hour(s) , time to finish: 4.3 hour(s)\n",
      "status: 47000000 , elapsed time: 3.725 hour(s) , time to finish: 4.2 hour(s)\n",
      "status: 48000000 , elapsed time: 3.788 hour(s) , time to finish: 4.1 hour(s)\n",
      "status: 49000000 , elapsed time: 3.859 hour(s) , time to finish: 4.0 hour(s)\n",
      "status: 50000000 , elapsed time: 3.919 hour(s) , time to finish: 3.9 hour(s)\n",
      "status: 51000000 , elapsed time: 3.982 hour(s) , time to finish: 3.8 hour(s)\n",
      "status: 52000000 , elapsed time: 4.044 hour(s) , time to finish: 3.7 hour(s)\n",
      "status: 53000000 , elapsed time: 4.105 hour(s) , time to finish: 3.6 hour(s)\n",
      "status: 54000000 , elapsed time: 4.181 hour(s) , time to finish: 3.6 hour(s)\n",
      "status: 55000000 , elapsed time: 4.253 hour(s) , time to finish: 3.5 hour(s)\n",
      "status: 56000000 , elapsed time: 4.322 hour(s) , time to finish: 3.4 hour(s)\n",
      "status: 57000000 , elapsed time: 4.398 hour(s) , time to finish: 3.3 hour(s)\n",
      "status: 58000000 , elapsed time: 4.47 hour(s) , time to finish: 3.2 hour(s)\n",
      "status: 59000000 , elapsed time: 4.548 hour(s) , time to finish: 3.2 hour(s)\n",
      "status: 60000000 , elapsed time: 4.614 hour(s) , time to finish: 3.1 hour(s)\n",
      "status: 61000000 , elapsed time: 4.681 hour(s) , time to finish: 3.0 hour(s)\n",
      "status: 62000000 , elapsed time: 4.759 hour(s) , time to finish: 2.9 hour(s)\n",
      "status: 63000000 , elapsed time: 4.83 hour(s) , time to finish: 2.8 hour(s)\n",
      "status: 64000000 , elapsed time: 4.903 hour(s) , time to finish: 2.8 hour(s)\n",
      "status: 65000000 , elapsed time: 4.974 hour(s) , time to finish: 2.7 hour(s)\n",
      "status: 66000000 , elapsed time: 5.048 hour(s) , time to finish: 2.6 hour(s)\n",
      "status: 67000000 , elapsed time: 5.113 hour(s) , time to finish: 2.5 hour(s)\n",
      "status: 68000000 , elapsed time: 5.184 hour(s) , time to finish: 2.4 hour(s)\n",
      "status: 69000000 , elapsed time: 5.252 hour(s) , time to finish: 2.4 hour(s)\n",
      "status: 70000000 , elapsed time: 5.327 hour(s) , time to finish: 2.3 hour(s)\n",
      "status: 71000000 , elapsed time: 5.397 hour(s) , time to finish: 2.2 hour(s)\n",
      "status: 72000000 , elapsed time: 5.467 hour(s) , time to finish: 2.1 hour(s)\n",
      "status: 73000000 , elapsed time: 5.547 hour(s) , time to finish: 2.1 hour(s)\n",
      "status: 74000000 , elapsed time: 5.62 hour(s) , time to finish: 2.0 hour(s)\n",
      "status: 75000000 , elapsed time: 5.689 hour(s) , time to finish: 1.9 hour(s)\n",
      "status: 76000000 , elapsed time: 5.768 hour(s) , time to finish: 1.8 hour(s)\n",
      "status: 77000000 , elapsed time: 5.833 hour(s) , time to finish: 1.7 hour(s)\n",
      "status: 78000000 , elapsed time: 5.898 hour(s) , time to finish: 1.7 hour(s)\n",
      "status: 79000000 , elapsed time: 5.964 hour(s) , time to finish: 1.6 hour(s)\n",
      "status: 80000000 , elapsed time: 6.025 hour(s) , time to finish: 1.5 hour(s)\n",
      "status: 81000000 , elapsed time: 6.092 hour(s) , time to finish: 1.4 hour(s)\n",
      "status: 82000000 , elapsed time: 6.163 hour(s) , time to finish: 1.4 hour(s)\n",
      "status: 83000000 , elapsed time: 6.226 hour(s) , time to finish: 1.3 hour(s)\n",
      "status: 84000000 , elapsed time: 6.292 hour(s) , time to finish: 1.2 hour(s)\n",
      "status: 85000000 , elapsed time: 6.362 hour(s) , time to finish: 1.1 hour(s)\n",
      "status: 86000000 , elapsed time: 6.441 hour(s) , time to finish: 1.0 hour(s)\n",
      "status: 87000000 , elapsed time: 6.516 hour(s) , time to finish: 1.0 hour(s)\n",
      "status: 88000000 , elapsed time: 6.597 hour(s) , time to finish: 0.9 hour(s)\n",
      "status: 89000000 , elapsed time: 6.668 hour(s) , time to finish: 0.8 hour(s)\n",
      "status: 90000000 , elapsed time: 6.737 hour(s) , time to finish: 0.7 hour(s)\n",
      "status: 91000000 , elapsed time: 6.803 hour(s) , time to finish: 0.7 hour(s)\n",
      "status: 92000000 , elapsed time: 6.862 hour(s) , time to finish: 0.6 hour(s)\n",
      "status: 93000000 , elapsed time: 6.92 hour(s) , time to finish: 0.5 hour(s)\n",
      "status: 94000000 , elapsed time: 6.98 hour(s) , time to finish: 0.4 hour(s)\n",
      "status: 95000000 , elapsed time: 7.038 hour(s) , time to finish: 0.4 hour(s)\n",
      "status: 96000000 , elapsed time: 7.096 hour(s) , time to finish: 0.3 hour(s)\n",
      "status: 97000000 , elapsed time: 7.154 hour(s) , time to finish: 0.2 hour(s)\n",
      "status: 98000000 , elapsed time: 7.211 hour(s) , time to finish: 0.1 hour(s)\n",
      "status: 99000000 , elapsed time: 7.27 hour(s) , time to finish: 0.1 hour(s)\n",
      "CPU times: user 7h 25min 25s, sys: 11.2 s, total: 7h 25min 37s\n",
      "Wall time: 7h 27min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "do_tweet_processing( 'World_twitterdata_part1.txt.gz', 'World_twitterdata_ready_part1.txt.gz' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
