{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does LDA do?\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "When I say topic, what is it actually and how it is represented?\n",
    "\n",
    "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "The following are key factors to obtaining good segregation topics:\n",
    "\n",
    "- The quality of text processing.\n",
    "- The variety of topics the text talks about.\n",
    "- The choice of topic modeling algorithm.\n",
    "- The number of topics fed to the algorithm.\n",
    "- The algorithms tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/abiricz/Storage2/ELTE/Fizika9félév/Data Science Computer Lab/Project2/dslab2018_project2\r\n",
      "../Data/All_filtered.txt  ../Data/All_filtered.txt.gz\r\n"
     ]
    }
   ],
   "source": [
    "!pwd && ls ../Data/All*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 25.7 ms, total: 389 ms\n",
      "Wall time: 392 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv( '../Data/All_filtered.txt.gz', compression='gzip', \n",
    "                    delimiter='\\t', names=['Tweets', 'Targets'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaah final saw thor even better than expect w...</td>\n",
       "      <td>cinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaahhh food benthanh street food market</td>\n",
       "      <td>market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaa am so happi succeed in buy ticket to the c...</td>\n",
       "      <td>cinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaand no question on climat or environ not su...</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaand on the job market too</td>\n",
       "      <td>market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Targets\n",
       "0  aaaah final saw thor even better than expect w...   cinema\n",
       "1           aaaahhh food benthanh street food market   market\n",
       "2  aaa am so happi succeed in buy ticket to the c...   cinema\n",
       "3  aaaand no question on climat or environ not su...  climate\n",
       "4                       aaaand on the job market too   market"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aaaah final saw thor even better than expect we laugh indec amount for finnish cinema ka raw you gem',\n",
       " 'cinema')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = data['Tweets'].values.tolist()\n",
    "targets = data['Targets'].values.tolist()\n",
    "tweets[0], targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 365 ms, sys: 52.2 ms, total: 417 ms\n",
      "Wall time: 415 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_words = []\n",
    "for i in range( len(tweets) ):\n",
    "    data_words.append( tweets[i].split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219941,), (219941,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_words), np.shape(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abiricz/.local/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaaah', 'final', 'saw', 'thor', 'even', 'better', 'than', 'expect', 'we', 'laugh', 'indec', 'amount', 'for', 'finnish', 'cinema', 'ka', 'raw', 'you', 'gem']\n",
      "CPU times: user 33.5 s, sys: 111 ms, total: 33.7 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 83.8 ms, total: 7 s\n",
      "Wall time: 7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219941,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( data_words_bigrams )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n",
      "CPU times: user 5.92 s, sys: 95.8 ms, total: 6.02 s\n",
      "Wall time: 6.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('aaaah', 1),\n",
       "  ('amount', 1),\n",
       "  ('better', 1),\n",
       "  ('cinema', 1),\n",
       "  ('even', 1),\n",
       "  ('expect', 1),\n",
       "  ('final', 1),\n",
       "  ('finnish', 1),\n",
       "  ('for', 1),\n",
       "  ('gem', 1),\n",
       "  ('indec', 1),\n",
       "  ('ka', 1),\n",
       "  ('laugh', 1),\n",
       "  ('raw', 1),\n",
       "  ('saw', 1),\n",
       "  ('than', 1),\n",
       "  ('thor', 1),\n",
       "  ('we', 1),\n",
       "  ('you', 1)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    975\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;31m# print out some debug info at the end of each EM iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36msync_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;34m\"\"\"Propagate the states topic probabilities to the inner object's attribute.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mPosterior\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the topics in LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.194*\"to\" + 0.130*\"and\" + 0.095*\"concert\" + 0.057*\"on\" + 0.054*\"this\" + '\n",
      "  '0.052*\"with\" + 0.037*\"was\" + 0.029*\"my\" + 0.022*\"year\" + 0.018*\"yeah\"'),\n",
      " (1,\n",
      "  '0.065*\"night\" + 0.038*\"hope\" + 0.038*\"give\" + 0.036*\"must\" + 0.034*\"parti\" '\n",
      "  '+ 0.031*\"alway\" + 0.029*\"its\" + 0.029*\"fan\" + 0.020*\"away\" + 0.017*\"fun\"'),\n",
      " (2,\n",
      "  '0.060*\"best\" + 0.049*\"ever\" + 0.039*\"use\" + 0.038*\"physic\" + 0.037*\"thing\" '\n",
      "  '+ 0.033*\"ll\" + 0.033*\"into\" + 0.031*\"being\" + 0.023*\"young\" + '\n",
      "  '0.021*\"without\"'),\n",
      " (3,\n",
      "  '0.101*\"see\" + 0.089*\"right\" + 0.083*\"time\" + 0.056*\"before\" + 0.023*\"open\" '\n",
      "  '+ 0.019*\"someon\" + 0.017*\"place\" + 0.016*\"head\" + 0.015*\"christma\" + '\n",
      "  '0.015*\"pay\"'),\n",
      " (4,\n",
      "  '0.143*\"go\" + 0.111*\"climat\" + 0.074*\"chang\" + 0.039*\"could\" + 0.031*\"wish\" '\n",
      "  '+ 0.029*\"let\" + 0.028*\"stop\" + 0.028*\"everi\" + 0.022*\"run\" + '\n",
      "  '0.019*\"american\"'),\n",
      " (5,\n",
      "  '0.102*\"me\" + 0.054*\"last\" + 0.053*\"never\" + 0.041*\"call\" + 0.026*\"amaz\" + '\n",
      "  '0.026*\"during\" + 0.022*\"better\" + 0.019*\"sure\" + 0.017*\"end\" + '\n",
      "  '0.016*\"person\"'),\n",
      " (6,\n",
      "  '0.103*\"cinema\" + 0.059*\"mean\" + 0.039*\"most\" + 0.029*\"miss\" + 0.028*\"white\" '\n",
      "  '+ 0.026*\"doing\" + 0.022*\"kid\" + 0.020*\"saw\" + 0.019*\"friend\" + '\n",
      "  '0.016*\"someth\"'),\n",
      " (7,\n",
      "  '0.068*\"has\" + 0.048*\"had\" + 0.036*\"take\" + 0.028*\"job\" + 0.027*\"where\" + '\n",
      "  '0.027*\"off\" + 0.022*\"made\" + 0.019*\"rig\" + 0.019*\"those\" + 0.018*\"put\"'),\n",
      " (8,\n",
      "  '0.093*\"need\" + 0.092*\"day\" + 0.056*\"guy\" + 0.030*\"week\" + 0.027*\"lose\" + '\n",
      "  '0.026*\"happen\" + 0.025*\"doesn\" + 0.018*\"whole\" + 0.017*\"school\" + '\n",
      "  '0.014*\"draft\"'),\n",
      " (9,\n",
      "  '0.181*\"nfl\" + 0.068*\"why\" + 0.052*\"no\" + 0.043*\"want\" + 0.041*\"there\" + '\n",
      "  '0.031*\"trump\" + 0.028*\"team\" + 0.024*\"realli\" + 0.024*\"back\" + '\n",
      "  '0.022*\"over\"'),\n",
      " (10,\n",
      "  '0.107*\"about\" + 0.063*\"should\" + 0.038*\"stock\" + 0.035*\"first\" + '\n",
      "  '0.028*\"any\" + 0.028*\"down\" + 0.023*\"went\" + 0.018*\"keep\" + 0.017*\"sinc\" + '\n",
      "  '0.017*\"tweet\"'),\n",
      " (11,\n",
      "  '0.150*\"terrorist\" + 0.088*\"terror\" + 0.039*\"were\" + 0.034*\"won\" + '\n",
      "  '0.020*\"ago\" + 0.018*\"talk\" + 0.015*\"kill\" + 0.014*\"polit\" + 0.013*\"against\" '\n",
      "  '+ 0.013*\"group\"'),\n",
      " (12,\n",
      "  '0.117*\"get\" + 0.039*\"here\" + 0.036*\"ticket\" + 0.035*\"live\" + 0.021*\"am\" + '\n",
      "  '0.018*\"hous\" + 0.016*\"old\" + 0.016*\"black\" + 0.015*\"both\" + 0.013*\"singl\"'),\n",
      " (13,\n",
      "  '0.146*\"your\" + 0.082*\"have\" + 0.072*\"yes\" + 0.049*\"so\" + 0.047*\"they\" + '\n",
      "  '0.046*\"like\" + 0.042*\"just\" + 0.030*\"how\" + 0.025*\"more\" + 0.025*\"wow\"'),\n",
      " (14,\n",
      "  '0.100*\"as\" + 0.075*\"win\" + 0.055*\"their\" + 0.033*\"new\" + 0.025*\"lost\" + '\n",
      "  '0.020*\"does\" + 0.018*\"post\" + 0.013*\"stupid\" + 0.012*\"hour\" + '\n",
      "  '0.011*\"product\"'),\n",
      " (15,\n",
      "  '0.170*\"can\" + 0.065*\"make\" + 0.050*\"next\" + 0.036*\"free\" + 0.033*\"support\" '\n",
      "  '+ 0.033*\"game\" + 0.032*\"other\" + 0.032*\"play\" + 0.024*\"man\" + '\n",
      "  '0.023*\"until\"'),\n",
      " (16,\n",
      "  '0.092*\"re\" + 0.085*\"out\" + 0.082*\"by\" + 0.072*\"or\" + 0.042*\"only\" + '\n",
      "  '0.037*\"got\" + 0.034*\"them\" + 0.029*\"world\" + 0.026*\"mani\" + 0.020*\"wrong\"'),\n",
      " (17,\n",
      "  '0.366*\"market\" + 0.084*\"would\" + 0.030*\"today\" + 0.030*\"yo\" + 0.026*\"yup\" + '\n",
      "  '0.020*\"yet\" + 0.019*\"start\" + 0.019*\"anoth\" + 0.018*\"money\" + 0.010*\"sell\"'),\n",
      " (18,\n",
      "  '0.167*\"the\" + 0.154*\"you\" + 0.067*\"in\" + 0.060*\"elect\" + 0.047*\"of\" + '\n",
      "  '0.046*\"is\" + 0.032*\"it\" + 0.032*\"that\" + 0.031*\"are\" + 0.022*\"be\"'),\n",
      " (19,\n",
      "  '0.158*\"for\" + 0.107*\"at\" + 0.063*\"all\" + 0.060*\"an\" + 0.047*\"who\" + '\n",
      "  '0.046*\"from\" + 0.041*\"one\" + 0.029*\"come\" + 0.028*\"his\" + 0.017*\"too\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint( lda_model.print_topics() )\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Model Perplexity and Coherence Score\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.170077735096758\n",
      "\n",
      "Coherence Score:  0.26106491807939797\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the topics-keywords\n",
    "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abiricz/.local/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el144871397104990165208891797142\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el144871397104990165208891797142_data = {\"mdsDat\": {\"x\": [0.38037362835661653, 0.23392937388842672, 0.059713131666674676, 0.12831617968055986, 0.02412756443071575, -0.0013014566397390554, -0.035305950478559535, -0.047986591207777596, -0.006389095127758092, -0.06836635842812241, -0.03388969131526458, -0.03645295124882074, -0.05783416838826857, -0.06414324508030311, -0.07623710394809177, -0.05174434342422306, -0.07990968163045332, -0.08796308520669521, -0.07889115243942921, -0.10004500345948711], \"y\": [-0.28681347187732303, 0.3746964616898526, 0.023624596310408395, 0.06289092357872297, 0.005085087750466164, -0.0006204598606513584, -0.009155337860255906, -0.010453203245986814, -0.0044094117253237275, -0.013054624952311423, -0.009454411158210546, -0.009629305635631061, -0.012274444009593191, -0.014237347798580073, -0.016294128441789135, -0.011373610615546146, -0.016650918199398023, -0.016878313394759932, -0.01643967785381647, -0.018558402700272338], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [24.907716751098633, 13.94643497467041, 6.3866987228393555, 6.1338372230529785, 4.6927666664123535, 4.687242031097412, 3.634150743484497, 3.2694220542907715, 3.2632853984832764, 3.1264376640319824, 2.930114269256592, 2.9275200366973877, 2.8775203227996826, 2.7730162143707275, 2.6724910736083984, 2.6666629314422607, 2.5143320560455322, 2.3279473781585693, 2.260944128036499, 2.0014541149139404]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"Freq\": [127281.0, 117624.0, 82905.0, 52513.0, 55661.0, 30979.0, 26065.0, 40416.0, 27498.0, 51379.0, 45952.0, 20924.0, 14998.0, 13914.0, 36045.0, 35405.0, 24353.0, 23146.0, 12789.0, 22400.0, 15392.0, 11924.0, 11202.0, 13445.0, 12029.0, 9981.0, 24789.0, 24758.0, 12388.0, 23906.0, 127280.2421875, 117623.3125, 51378.61328125, 45951.8359375, 36044.98046875, 35404.99609375, 24757.7890625, 24788.1796875, 23905.810546875, 16637.224609375, 13961.62109375, 13901.2626953125, 13046.251953125, 12130.0166015625, 11428.908203125, 10534.8486328125, 10235.9853515625, 8989.22265625, 8794.1220703125, 8722.248046875, 7843.13818359375, 6582.49169921875, 6500.03662109375, 6396.51220703125, 6110.84423828125, 5671.150390625, 5255.00048828125, 4158.43701171875, 4154.8271484375, 3990.85791015625, 82904.3359375, 55660.33203125, 40416.0625, 24352.40625, 23145.642578125, 22399.12890625, 15941.3857421875, 12398.0283203125, 9483.8251953125, 7494.220703125, 5372.83251953125, 4650.4208984375, 4590.138671875, 4075.449951171875, 4023.021240234375, 3946.6591796875, 3318.378662109375, 3140.243408203125, 2714.22265625, 2679.54736328125, 2617.70849609375, 2604.829833984375, 2520.673583984375, 2504.2919921875, 2212.926025390625, 2177.32763671875, 2166.25146484375, 1985.0621337890625, 1622.1856689453125, 1580.6541748046875, 30978.978515625, 20923.587890625, 12387.337890625, 11832.9765625, 9160.8359375, 8948.9404296875, 7968.708984375, 5730.1484375, 5437.17236328125, 3398.40966796875, 3358.3798828125, 2646.972412109375, 2159.207275390625, 2143.620849609375, 1973.3878173828125, 1770.7305908203125, 1536.837646484375, 1409.65234375, 1400.7501220703125, 1211.6837158203125, 1205.372314453125, 1187.2164306640625, 1043.724365234375, 1035.3267822265625, 841.9290161132812, 812.0584106445312, 809.73095703125, 808.0382690429688, 751.3207397460938, 722.3486938476562, 2931.451416015625, 27497.771484375, 15391.9248046875, 13444.2451171875, 9185.3056640625, 8894.9580078125, 8571.6875, 7938.07373046875, 5652.07958984375, 4666.93701171875, 4617.33447265625, 4439.32666015625, 3845.71337890625, 3310.038330078125, 3175.72900390625, 2964.671142578125, 2683.462158203125, 2202.68896484375, 2077.686279296875, 2067.970458984375, 1460.99462890625, 1416.8607177734375, 1186.452392578125, 1174.509521484375, 1069.650146484375, 993.528564453125, 985.4383544921875, 897.7216186523438, 884.6154174804688, 883.478271484375, 869.063232421875, 26064.474609375, 9841.150390625, 7415.42822265625, 6123.78173828125, 5890.84814453125, 4510.07958984375, 4050.060791015625, 3472.11181640625, 3396.40087890625, 3179.474609375, 3145.54443359375, 3023.1220703125, 2292.27001953125, 1829.6177978515625, 1822.22900390625, 1691.376220703125, 1464.5379638671875, 1424.545166015625, 1177.027587890625, 1037.4586181640625, 1035.3494873046875, 1006.3789672851562, 951.1126098632812, 829.7387084960938, 772.0364379882812, 756.7003173828125, 747.054443359375, 746.8968505859375, 736.6825561523438, 669.6845703125, 52512.45703125, 12028.9638671875, 4301.59765625, 4286.67138671875, 3716.30859375, 2819.941162109375, 2769.256591796875, 2670.2236328125, 2530.1689453125, 1428.08203125, 1405.78466796875, 1315.9144287109375, 1287.100341796875, 934.1881103515625, 829.0205688476562, 827.3197631835938, 766.922607421875, 748.6785278320312, 740.9482421875, 699.4686279296875, 651.1539306640625, 613.7280883789062, 604.3344116210938, 577.0265502929688, 545.3426513671875, 524.5145874023438, 478.6928405761719, 478.4444580078125, 450.6849670410156, 421.71600341796875, 11923.9482421875, 7045.6416015625, 4274.6650390625, 3931.30126953125, 3155.957275390625, 3117.151123046875, 2524.76513671875, 1996.7210693359375, 1943.75048828125, 1848.0706787109375, 1645.155029296875, 1331.9630126953125, 1330.917236328125, 1232.372802734375, 1221.0491943359375, 1216.2828369140625, 1210.4169921875, 1137.228271484375, 1102.8416748046875, 1099.6553955078125, 1092.3662109375, 1023.4006958007812, 991.787841796875, 987.4615478515625, 967.3015747070312, 960.5325927734375, 958.3651733398438, 953.0739135742188, 953.0864868164062, 865.23779296875, 6829.4541015625, 4835.31298828125, 3602.147705078125, 2849.001953125, 2752.125, 2664.195556640625, 2154.017822265625, 1914.66552734375, 1866.1719970703125, 1847.5540771484375, 1607.69287109375, 1482.747802734375, 1447.4989013671875, 1352.7462158203125, 1335.7208251953125, 1271.4852294921875, 1262.993408203125, 1190.366943359375, 1038.34228515625, 962.5473022460938, 929.3359985351562, 870.924560546875, 863.30615234375, 811.1739501953125, 767.6849365234375, 760.5421142578125, 756.9027709960938, 755.72705078125, 728.6109008789062, 691.7759399414062, 14997.0810546875, 8798.5, 3857.781005859375, 3368.52294921875, 2019.4066162109375, 1751.6676025390625, 1542.5521240234375, 1435.2264404296875, 1347.001708984375, 1281.788330078125, 1245.8436279296875, 1174.883056640625, 1172.6356201171875, 1072.59228515625, 1023.4759521484375, 844.8807373046875, 778.4028930664062, 746.5631103515625, 740.3876342773438, 726.0194702148438, 697.8223266601562, 682.2036743164062, 643.2281494140625, 641.8494873046875, 615.0238037109375, 579.9274291992188, 550.2711791992188, 500.4302978515625, 492.0326232910156, 484.85400390625, 11201.248046875, 3746.689453125, 3494.60546875, 3395.516357421875, 2013.7803955078125, 1708.1983642578125, 1496.8466796875, 1492.275390625, 1473.05078125, 1266.4422607421875, 1264.2657470703125, 1244.2996826171875, 1209.063720703125, 1099.921875, 1094.8197021484375, 1070.674072265625, 1039.1534423828125, 962.8326416015625, 946.219970703125, 918.832275390625, 915.2664184570312, 858.281005859375, 848.5643920898438, 799.225341796875, 792.7518920898438, 778.8809814453125, 774.1841430664062, 739.7017211914062, 705.7050170898438, 694.31640625, 8233.7783203125, 7595.83203125, 7335.18115234375, 6443.7373046875, 3766.045654296875, 3292.380859375, 3021.82373046875, 2570.702880859375, 2301.5126953125, 1790.623046875, 1707.6373291015625, 1562.397705078125, 1447.5048828125, 1399.6292724609375, 975.74609375, 962.6763916015625, 787.5703125, 732.0682983398438, 692.9407958984375, 675.7891235351562, 636.2042846679688, 622.1842651367188, 604.118408203125, 588.608154296875, 581.3942260742188, 566.5575561523438, 555.375244140625, 511.0782775878906, 475.2377624511719, 455.5153503417969, 12788.1240234375, 9980.564453125, 6606.51318359375, 3452.476806640625, 2748.802001953125, 2609.067626953125, 2472.47265625, 2470.18017578125, 1957.8824462890625, 1669.2679443359375, 1349.018798828125, 1185.636962890625, 919.71826171875, 887.117431640625, 783.33935546875, 713.3267211914062, 712.827880859375, 696.2281494140625, 663.7213745117188, 621.6961059570312, 562.6758422851562, 500.4354553222656, 494.6308288574219, 487.22216796875, 479.32049560546875, 479.1047668457031, 442.10662841796875, 426.72344970703125, 419.58953857421875, 402.97222900390625, 9000.171875, 4743.05322265625, 4640.38720703125, 3593.034423828125, 2331.200439453125, 2320.408447265625, 1955.8671875, 1679.4027099609375, 1507.9005126953125, 1443.93505859375, 1277.3173828125, 1238.8841552734375, 1108.5703125, 1101.8349609375, 1009.4620971679688, 964.3310546875, 933.3215942382812, 879.524169921875, 856.9011840820312, 853.4664306640625, 827.5033569335938, 730.730224609375, 675.47998046875, 653.7125244140625, 636.9227294921875, 605.9446411132812, 580.963134765625, 576.8402099609375, 547.6322021484375, 535.2600708007812, 8533.4306640625, 6359.4609375, 4694.9921875, 2790.540283203125, 2165.4970703125, 1685.827880859375, 1520.9642333984375, 1122.8973388671875, 980.9073486328125, 951.8643798828125, 874.313720703125, 838.8863525390625, 826.04541015625, 756.7603149414062, 756.6574096679688, 734.7036743164062, 705.681396484375, 666.606689453125, 658.264404296875, 634.6122436523438, 628.2367553710938, 615.9990844726562, 602.091064453125, 586.1697387695312, 580.7431030273438, 548.7686157226562, 548.0848388671875, 522.619140625, 486.78521728515625, 471.8258361816406, 8226.25, 7250.947265625, 6825.4111328125, 4567.85498046875, 1914.685546875, 1531.0841064453125, 1361.3721923828125, 1332.496337890625, 1267.4910888671875, 1257.042236328125, 1105.4774169921875, 1004.5897827148438, 882.0631103515625, 865.313232421875, 846.619384765625, 815.2687377929688, 758.2648315429688, 750.33056640625, 631.8566284179688, 629.5596923828125, 582.2926025390625, 559.0985717773438, 557.06494140625, 555.9381713867188, 534.5291137695312, 528.6483154296875, 512.1103515625, 494.7773132324219, 491.7627868652344, 479.1857604980469, 13913.92578125, 5348.4140625, 4079.040283203125, 2922.359375, 2717.052001953125, 2705.101806640625, 2611.808349609375, 2592.06982421875, 1983.0391845703125, 1841.4967041015625, 1515.13427734375, 805.2860717773438, 751.7236938476562, 728.6296997070312, 714.8746948242188, 699.3027954101562, 697.7557983398438, 649.30810546875, 621.3373413085938, 564.051025390625, 510.01910400390625, 502.16259765625, 491.0984802246094, 485.5558166503906, 483.0882873535156, 464.7765808105469, 443.1670227050781, 422.35589599609375, 415.1030578613281, 378.10894775390625, 7131.68115234375, 7080.73388671875, 4287.0703125, 2322.311767578125, 2088.69775390625, 1993.0977783203125, 1921.3968505859375, 1407.4071044921875, 1298.855712890625, 1049.5921630859375, 1038.498291015625, 999.330078125, 835.4815063476562, 755.626708984375, 733.8143920898438, 699.7910766601562, 684.2975463867188, 657.8169555664062, 635.1748046875, 578.95068359375, 573.6141967773438, 570.3136596679688, 569.5250244140625, 562.8732299804688, 545.7631225585938, 532.283203125, 519.4291381835938, 491.8386535644531, 468.4981384277344, 456.7538146972656, 7312.5771484375, 4242.15966796875, 2768.54150390625, 2060.505615234375, 2026.135009765625, 1854.4188232421875, 1546.0496826171875, 1450.0491943359375, 1353.200927734375, 1148.5394287109375, 1070.770263671875, 928.2787475585938, 877.7647094726562, 867.3182373046875, 845.8073120117188, 784.9138793945312, 773.264892578125, 770.115234375, 662.7789916992188, 658.9232177734375, 478.084716796875, 467.1152648925781, 439.2339782714844, 398.39727783203125, 397.1860656738281, 373.4090881347656, 369.8838806152344, 366.1744079589844, 365.9910583496094, 332.642333984375, 4136.74365234375, 3393.17041015625, 2676.966552734375, 2611.67822265625, 2543.67724609375, 2272.135498046875, 2259.187255859375, 2157.4677734375, 1564.316162109375, 1445.0885009765625, 1357.2984619140625, 1149.8988037109375, 843.66650390625, 794.5173950195312, 765.0662841796875, 659.6675415039062, 624.7525634765625, 581.9072875976562, 557.9027709960938, 494.3272705078125, 481.0090026855469, 443.8359680175781, 442.21002197265625, 420.68902587890625, 419.0207214355469, 418.95648193359375, 414.7850646972656, 411.84979248046875, 388.118896484375, 364.2740478515625, 3961.369384765625, 2321.506591796875, 2311.01220703125, 2233.58447265625, 2069.02490234375, 1915.7352294921875, 1763.908447265625, 1747.412841796875, 1232.02587890625, 1070.4964599609375, 933.2185668945312, 822.904052734375, 801.6898193359375, 677.2464599609375, 663.6340942382812, 641.1378784179688, 588.660888671875, 579.3821411132812, 527.0884399414062, 459.75604248046875, 454.74249267578125, 438.5171813964844, 399.82147216796875, 395.78289794921875, 388.89324951171875, 386.6033630371094, 385.9127197265625, 372.7371826171875, 339.45648193359375, 333.0267028808594], \"Term\": [\"the\", \"you\", \"to\", \"market\", \"and\", \"for\", \"nfl\", \"concert\", \"your\", \"in\", \"elect\", \"at\", \"terrorist\", \"can\", \"of\", \"is\", \"on\", \"this\", \"go\", \"with\", \"have\", \"about\", \"get\", \"yes\", \"would\", \"climat\", \"it\", \"that\", \"all\", \"are\", \"the\", \"you\", \"in\", \"elect\", \"of\", \"is\", \"that\", \"it\", \"are\", \"be\", \"we\", \"not\", \"will\", \"know\", \"but\", \"when\", \"what\", \"if\", \"he\", \"do\", \"up\", \"peopl\", \"don\", \"our\", \"think\", \"now\", \"vote\", \"us\", \"still\", \"say\", \"to\", \"and\", \"concert\", \"on\", \"this\", \"with\", \"was\", \"my\", \"year\", \"yeah\", \"watch\", \"love\", \"work\", \"good\", \"because\", \"ve\", \"some\", \"show\", \"thank\", \"also\", \"way\", \"yesterday\", \"didn\", \"she\", \"very\", \"her\", \"pleas\", \"which\", \"seen\", \"gonna\", \"for\", \"at\", \"all\", \"an\", \"who\", \"from\", \"one\", \"come\", \"his\", \"too\", \"look\", \"countri\", \"same\", \"again\", \"wait\", \"find\", \"nation\", \"point\", \"home\", \"turn\", \"tomorrow\", \"anyth\", \"forget\", \"bring\", \"god\", \"zimbabw\", \"hand\", \"nba\", \"full\", \"bc\", \"yep\", \"your\", \"have\", \"yes\", \"so\", \"they\", \"like\", \"just\", \"how\", \"more\", \"wow\", \"been\", \"after\", \"did\", \"tri\", \"even\", \"than\", \"much\", \"tonight\", \"help\", \"two\", \"believ\", \"while\", \"feel\", \"alreadi\", \"caus\", \"video\", \"isn\", \"idiot\", \"organ\", \"wtf\", \"nfl\", \"why\", \"no\", \"want\", \"there\", \"trump\", \"team\", \"realli\", \"back\", \"over\", \"him\", \"great\", \"player\", \"presid\", \"real\", \"big\", \"wouldn\", \"read\", \"probabl\", \"footbal\", \"major\", \"qb\", \"allow\", \"left\", \"biggest\", \"zero\", \"fire\", \"blame\", \"influenc\", \"wasn\", \"market\", \"would\", \"today\", \"yo\", \"yup\", \"yet\", \"start\", \"anoth\", \"money\", \"sell\", \"pick\", \"busi\", \"plan\", \"news\", \"street\", \"idea\", \"base\", \"invest\", \"learn\", \"direct\", \"futur\", \"la\", \"earli\", \"success\", \"past\", \"opportun\", \"spend\", \"cost\", \"strategi\", \"social_media\", \"about\", \"should\", \"stock\", \"first\", \"any\", \"down\", \"went\", \"keep\", \"sinc\", \"tweet\", \"buy\", \"ya\", \"america\", \"close\", \"histori\", \"hire\", \"ask\", \"yea\", \"pass\", \"boy\", \"problem\", \"youth\", \"top\", \"hungari\", \"th\", \"marketplac\", \"interest\", \"drop\", \"import\", \"morn\", \"has\", \"had\", \"take\", \"job\", \"where\", \"off\", \"made\", \"rig\", \"those\", \"put\", \"worst\", \"actual\", \"presidenti\", \"democrat\", \"lot\", \"local\", \"word\", \"campaign\", \"special\", \"record\", \"mind\", \"agre\", \"everyth\", \"sir\", \"abl\", \"absolut\", \"true\", \"worth\", \"enough\", \"write\", \"terrorist\", \"terror\", \"were\", \"won\", \"ago\", \"talk\", \"kill\", \"polit\", \"against\", \"group\", \"act\", \"domest\", \"life\", \"long\", \"hate\", \"muslim\", \"welcom\", \"target\", \"definit\", \"name\", \"fear\", \"citi\", \"creat\", \"anti\", \"came\", \"war\", \"islam\", \"florida\", \"view\", \"isi\", \"get\", \"here\", \"ticket\", \"live\", \"am\", \"hous\", \"old\", \"black\", \"both\", \"singl\", \"farmer\", \"part\", \"happi\", \"perform\", \"music\", \"sunday\", \"having\", \"price\", \"saturday\", \"enjoy\", \"correct\", \"stori\", \"though\", \"sold\", \"london\", \"band\", \"brand\", \"global\", \"wanna\", \"lead\", \"re\", \"out\", \"by\", \"or\", \"only\", \"got\", \"them\", \"world\", \"mani\", \"wrong\", \"tell\", \"own\", \"noth\", \"russian\", \"follow\", \"govern\", \"heard\", \"share\", \"check\", \"forc\", \"hold\", \"weather\", \"deal\", \"almost\", \"either\", \"clear\", \"exact\", \"fund\", \"facebook\", \"quit\", \"go\", \"climat\", \"chang\", \"could\", \"wish\", \"let\", \"stop\", \"everi\", \"run\", \"american\", \"obama\", \"care\", \"gotta\", \"stay\", \"gun\", \"focus\", \"protect\", \"gone\", \"hard\", \"novemb\", \"knew\", \"respons\", \"half\", \"threat\", \"late\", \"cut\", \"alon\", \"economi\", \"crime\", \"fraud\", \"me\", \"last\", \"never\", \"call\", \"amaz\", \"during\", \"better\", \"sure\", \"end\", \"person\", \"done\", \"around\", \"thought\", \"readi\", \"leader\", \"sound\", \"candid\", \"current\", \"kind\", \"through\", \"walk\", \"rule\", \"voter\", \"number\", \"dont\", \"cannot\", \"usa\", \"low\", \"second\", \"senat\", \"as\", \"win\", \"their\", \"new\", \"lost\", \"does\", \"post\", \"stupid\", \"hour\", \"product\", \"power\", \"media\", \"dem\", \"pm\", \"event\", \"poll\", \"crash\", \"liber\", \"network\", \"colleg\", \"told\", \"especi\", \"join\", \"fool\", \"complet\", \"rock\", \"student\", \"car\", \"cours\", \"behind\", \"see\", \"right\", \"time\", \"before\", \"open\", \"someon\", \"place\", \"head\", \"christma\", \"pay\", \"leav\", \"high\", \"pretti\", \"famili\", \"food\", \"manag\", \"winter\", \"matter\", \"suck\", \"voic\", \"die\", \"member\", \"corrupt\", \"ur\", \"small\", \"messag\", \"mouth\", \"each\", \"rais\", \"concern\", \"can\", \"make\", \"next\", \"free\", \"support\", \"game\", \"other\", \"play\", \"man\", \"until\", \"result\", \"control\", \"fair\", \"sing\", \"catch\", \"beat\", \"awesom\", \"includ\", \"congress\", \"paid\", \"tv\", \"illeg\", \"coach\", \"throw\", \"couldn\", \"instead\", \"primari\", \"eat\", \"announc\", \"pop\", \"need\", \"day\", \"guy\", \"week\", \"lose\", \"happen\", \"doesn\", \"whole\", \"school\", \"draft\", \"month\", \"report\", \"film\", \"face\", \"claim\", \"secur\", \"ad\", \"remind\", \"yesss\", \"hell\", \"sale\", \"sport\", \"togeth\", \"investig\", \"hall\", \"oh\", \"shoot\", \"suppos\", \"book\", \"least\", \"cinema\", \"mean\", \"most\", \"miss\", \"white\", \"doing\", \"kid\", \"saw\", \"friend\", \"someth\", \"stand\", \"soon\", \"song\", \"meet\", \"law\", \"expect\", \"favorit\", \"excit\", \"final\", \"bet\", \"artist\", \"zombi\", \"cycl\", \"upcom\", \"featur\", \"ma\", \"trip\", \"moment\", \"region\", \"wast\", \"best\", \"ever\", \"use\", \"physic\", \"thing\", \"ll\", \"into\", \"being\", \"young\", \"without\", \"hear\", \"move\", \"issu\", \"protest\", \"total\", \"count\", \"valu\", \"uk\", \"impact\", \"woman\", \"anymor\", \"talent\", \"consid\", \"etc\", \"fake\", \"room\", \"board\", \"honest\", \"sorri\", \"billion\", \"night\", \"hope\", \"give\", \"must\", \"parti\", \"alway\", \"its\", \"fan\", \"away\", \"fun\", \"differ\", \"far\", \"line\", \"mr\", \"twitter\", \"cinematographi\", \"wear\", \"fall\", \"accept\", \"met\", \"st\", \"blue\", \"custom\", \"shouldn\", \"church\", \"wrote\", \"critic\", \"cook\", \"along\", \"melbourn\"], \"Total\": [127281.0, 117624.0, 82905.0, 52513.0, 55661.0, 30979.0, 26065.0, 40416.0, 27498.0, 51379.0, 45952.0, 20924.0, 14998.0, 13914.0, 36045.0, 35405.0, 24353.0, 23146.0, 12789.0, 22400.0, 15392.0, 11924.0, 11202.0, 13445.0, 12029.0, 9981.0, 24789.0, 24758.0, 12388.0, 23906.0, 127281.171875, 117624.2421875, 51379.53515625, 45952.7578125, 36045.90234375, 35405.91796875, 24758.712890625, 24789.103515625, 23906.734375, 16638.1484375, 13962.5517578125, 13902.193359375, 13047.1826171875, 12130.947265625, 11429.8388671875, 10535.779296875, 10236.916015625, 8990.1533203125, 8795.052734375, 8723.1787109375, 7844.06591796875, 6583.41943359375, 6500.96435546875, 6397.43994140625, 6111.77197265625, 5672.078125, 5255.92822265625, 4159.36474609375, 4155.7548828125, 3991.7861328125, 82905.2578125, 55661.24609375, 40416.9765625, 24353.322265625, 23146.55859375, 22400.044921875, 15942.3095703125, 12398.9521484375, 9484.7490234375, 7495.14111328125, 5373.7529296875, 4651.34130859375, 4591.05908203125, 4076.370849609375, 4023.942138671875, 3947.580078125, 3319.299560546875, 3141.164306640625, 2715.1435546875, 2680.46826171875, 2618.62939453125, 2605.750732421875, 2521.594482421875, 2505.212890625, 2213.846923828125, 2178.24853515625, 2167.17236328125, 1985.9832763671875, 1623.1068115234375, 1581.5753173828125, 30979.89453125, 20924.50390625, 12388.259765625, 11833.8984375, 9161.7578125, 8949.8623046875, 7969.62841796875, 5731.06787109375, 5438.091796875, 3399.329345703125, 3359.299560546875, 2647.89208984375, 2160.126953125, 2144.54052734375, 1974.3077392578125, 1771.6505126953125, 1537.757568359375, 1410.572265625, 1401.6700439453125, 1212.6036376953125, 1206.292236328125, 1188.1363525390625, 1044.644287109375, 1036.2467041015625, 842.848876953125, 812.978271484375, 810.6508178710938, 808.9581298828125, 752.2406005859375, 723.2685546875, 2968.2626953125, 27498.69921875, 15392.8583984375, 13445.1787109375, 9186.2392578125, 8895.8916015625, 8572.62109375, 7939.00439453125, 5653.01025390625, 4667.86767578125, 4618.26513671875, 4440.25732421875, 3846.644775390625, 3310.9697265625, 3176.660400390625, 2965.6025390625, 2684.3935546875, 2203.620361328125, 2078.61767578125, 2068.90185546875, 1461.926025390625, 1417.7921142578125, 1187.3837890625, 1175.44091796875, 1070.58154296875, 994.4599609375, 986.3697509765625, 898.6530151367188, 885.5468139648438, 884.40966796875, 869.99462890625, 26065.392578125, 9842.0751953125, 7416.35009765625, 6124.70361328125, 5891.77001953125, 4511.00146484375, 4050.9833984375, 3473.034423828125, 3397.323486328125, 3180.397216796875, 3146.467041015625, 3024.044677734375, 2293.192626953125, 1830.54052734375, 1823.1517333984375, 1692.2989501953125, 1465.460693359375, 1425.4678955078125, 1177.9503173828125, 1038.38134765625, 1036.272216796875, 1007.3016357421875, 952.0352783203125, 830.661376953125, 772.9591064453125, 757.6229858398438, 747.9771118164062, 747.8195190429688, 737.605224609375, 670.6072387695312, 52513.375, 12029.8857421875, 4302.51708984375, 4287.5908203125, 3717.228271484375, 2820.86083984375, 2770.17626953125, 2671.143310546875, 2531.088623046875, 1429.0018310546875, 1406.7044677734375, 1316.834228515625, 1288.0201416015625, 935.10791015625, 829.9403686523438, 828.2395629882812, 767.8424072265625, 749.5983276367188, 741.8680419921875, 700.388427734375, 652.07373046875, 614.6478881835938, 605.2542114257812, 577.9463500976562, 546.262451171875, 525.4343872070312, 479.6125793457031, 479.36419677734375, 451.6047058105469, 422.6357421875, 11924.869140625, 7046.56005859375, 4275.58349609375, 3932.220458984375, 3156.87646484375, 3118.0703125, 2525.684326171875, 1997.640380859375, 1944.6697998046875, 1848.989990234375, 1646.0743408203125, 1332.88232421875, 1331.8365478515625, 1233.2921142578125, 1221.968505859375, 1217.2021484375, 1211.3363037109375, 1138.1475830078125, 1103.760986328125, 1100.57470703125, 1093.2855224609375, 1024.320068359375, 992.7070922851562, 988.3807983398438, 968.2208251953125, 961.4518432617188, 959.284423828125, 953.9931640625, 954.0057373046875, 866.1570434570312, 6830.373046875, 4836.23193359375, 3603.0673828125, 2849.921630859375, 2753.044677734375, 2665.115234375, 2154.9375, 1915.5853271484375, 1867.091796875, 1848.473876953125, 1608.6126708984375, 1483.6676025390625, 1448.418701171875, 1353.666015625, 1336.640625, 1272.405029296875, 1263.9132080078125, 1191.2867431640625, 1039.2620849609375, 963.4669799804688, 930.2556762695312, 871.84423828125, 864.225830078125, 812.0936279296875, 768.6046142578125, 761.4617919921875, 757.8224487304688, 756.646728515625, 729.5305786132812, 692.6956176757812, 14998.013671875, 8799.4326171875, 3858.710693359375, 3369.45263671875, 2020.3365478515625, 1752.5975341796875, 1543.4820556640625, 1436.1563720703125, 1347.931640625, 1282.71826171875, 1246.7735595703125, 1175.81298828125, 1173.5655517578125, 1073.522216796875, 1024.4058837890625, 845.8104858398438, 779.3326416015625, 747.4928588867188, 741.3173828125, 726.94921875, 698.7520751953125, 683.1334228515625, 644.1578979492188, 642.7792358398438, 615.9535522460938, 580.857177734375, 551.200927734375, 501.3600158691406, 492.96234130859375, 485.7837219238281, 11202.1669921875, 3747.60546875, 3495.521484375, 3396.432373046875, 2014.6966552734375, 1709.1146240234375, 1497.762939453125, 1493.191650390625, 1473.967041015625, 1267.3585205078125, 1265.1820068359375, 1245.2159423828125, 1209.97998046875, 1100.838134765625, 1095.7359619140625, 1071.59033203125, 1040.0697021484375, 963.7488403320312, 947.1361694335938, 919.7484741210938, 916.1826171875, 859.1972045898438, 849.4805908203125, 800.1415405273438, 793.6680908203125, 779.7971801757812, 775.100341796875, 740.617919921875, 706.6212158203125, 695.2326049804688, 8234.7080078125, 7596.7587890625, 7336.10791015625, 6444.6640625, 3766.97314453125, 3293.308349609375, 3022.751220703125, 2571.63037109375, 2302.440185546875, 1791.55078125, 1708.5650634765625, 1563.325439453125, 1448.4326171875, 1400.5570068359375, 976.6736450195312, 963.6039428710938, 788.4978637695312, 732.995849609375, 693.8683471679688, 676.7166748046875, 637.1318359375, 623.11181640625, 605.0459594726562, 589.5357055664062, 582.32177734375, 567.485107421875, 556.3027954101562, 512.0057983398438, 476.165283203125, 456.44287109375, 12789.05078125, 9981.4912109375, 6607.43701171875, 3453.401611328125, 2749.726806640625, 2609.992431640625, 2473.3974609375, 2471.10498046875, 1958.807373046875, 1670.19287109375, 1349.9437255859375, 1186.5618896484375, 920.64306640625, 888.042236328125, 784.26416015625, 714.2515258789062, 713.752685546875, 697.1529541015625, 664.6461791992188, 622.6209106445312, 563.6006469726562, 501.3602294921875, 495.55560302734375, 488.1469421386719, 480.2452697753906, 480.029541015625, 443.0314025878906, 427.6482238769531, 420.5143127441406, 403.8970031738281, 9001.0966796875, 4743.97509765625, 4641.30908203125, 3593.95703125, 2332.123046875, 2321.3310546875, 1956.7901611328125, 1680.32568359375, 1508.823486328125, 1444.8580322265625, 1278.2403564453125, 1239.80712890625, 1109.4932861328125, 1102.7579345703125, 1010.3848876953125, 965.2538452148438, 934.244384765625, 880.4469604492188, 857.823974609375, 854.3892211914062, 828.4261474609375, 731.6530151367188, 676.4027709960938, 654.6353149414062, 637.8455200195312, 606.867431640625, 581.8859252929688, 577.7630004882812, 548.5549926757812, 536.182861328125, 8534.353515625, 6360.3818359375, 4695.9130859375, 2791.461669921875, 2166.41845703125, 1686.7496337890625, 1521.885986328125, 1123.819091796875, 981.8289184570312, 952.7859497070312, 875.2352905273438, 839.8079223632812, 826.9669799804688, 757.681884765625, 757.5789794921875, 735.625244140625, 706.6029663085938, 667.5282592773438, 659.1859741210938, 635.5338134765625, 629.1583251953125, 616.920654296875, 603.0126342773438, 587.09130859375, 581.6646728515625, 549.690185546875, 549.0064086914062, 523.5407104492188, 487.7067565917969, 472.74737548828125, 8227.169921875, 7251.865234375, 6826.3291015625, 4568.77294921875, 1915.6046142578125, 1532.003173828125, 1362.291259765625, 1333.4154052734375, 1268.41015625, 1257.9613037109375, 1106.396484375, 1005.5087280273438, 882.9820556640625, 866.232177734375, 847.538330078125, 816.1876831054688, 759.1837768554688, 751.24951171875, 632.7755737304688, 630.4786376953125, 583.2115478515625, 560.0175170898438, 557.98388671875, 556.8571166992188, 535.4480590820312, 529.5672607421875, 513.029296875, 495.6961975097656, 492.6816711425781, 480.1046447753906, 13914.8525390625, 5349.33935546875, 4079.966064453125, 2923.28515625, 2717.977783203125, 2706.027587890625, 2612.734130859375, 2592.99560546875, 1983.965087890625, 1842.422607421875, 1516.0601806640625, 806.2118530273438, 752.6494750976562, 729.5554809570312, 715.8004760742188, 700.2285766601562, 698.6815795898438, 650.23388671875, 622.2631225585938, 564.976806640625, 510.9448547363281, 503.0883483886719, 492.02423095703125, 486.4815673828125, 484.0140380859375, 465.70233154296875, 444.0927734375, 423.2816467285156, 416.02880859375, 379.0346984863281, 7132.6005859375, 7081.6533203125, 4287.98974609375, 2323.231689453125, 2089.61767578125, 1994.017578125, 1922.316650390625, 1408.326904296875, 1299.7755126953125, 1050.511962890625, 1039.4180908203125, 1000.2498168945312, 836.4012451171875, 756.5464477539062, 734.734130859375, 700.7108154296875, 685.21728515625, 658.7366943359375, 636.0946044921875, 579.8704223632812, 574.533935546875, 571.2333984375, 570.4447631835938, 563.79296875, 546.682861328125, 533.2029418945312, 520.348876953125, 492.75836181640625, 469.4178466796875, 457.67352294921875, 7313.49755859375, 4243.080078125, 2769.46240234375, 2061.426513671875, 2027.0557861328125, 1855.339599609375, 1546.970458984375, 1450.969970703125, 1354.1217041015625, 1149.460205078125, 1071.6910400390625, 929.199462890625, 878.6854248046875, 868.2389526367188, 846.72802734375, 785.8345947265625, 774.1856079101562, 771.0359497070312, 663.69970703125, 659.8439331054688, 479.00543212890625, 468.0359802246094, 440.1546936035156, 399.3179931640625, 398.1067810058594, 374.3298034667969, 370.8045959472656, 367.0951232910156, 366.9117736816406, 333.56304931640625, 4137.6669921875, 3394.09375, 2677.889892578125, 2612.6015625, 2544.6005859375, 2273.058837890625, 2260.110595703125, 2158.39111328125, 1565.2396240234375, 1446.011962890625, 1358.221923828125, 1150.822265625, 844.5899658203125, 795.4408569335938, 765.98974609375, 660.5910034179688, 625.676025390625, 582.8307495117188, 558.8262329101562, 495.2507019042969, 481.93243408203125, 444.7593994140625, 443.1334533691406, 421.6124572753906, 419.94415283203125, 419.8799133300781, 415.70849609375, 412.7732238769531, 389.0423278808594, 365.1974792480469, 3962.2900390625, 2322.42724609375, 2311.932861328125, 2234.505126953125, 2069.945556640625, 1916.656005859375, 1764.8292236328125, 1748.3336181640625, 1232.9466552734375, 1071.417236328125, 934.1392822265625, 823.8247680664062, 802.6105346679688, 678.1671752929688, 664.5548095703125, 642.05859375, 589.5816040039062, 580.3028564453125, 528.0091552734375, 460.6767578125, 455.6632080078125, 439.4378967285156, 400.7421875, 396.70361328125, 389.81396484375, 387.5240783691406, 386.83343505859375, 373.65789794921875, 340.377197265625, 333.9474182128906], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3899999856948853, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.3898999691009521, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.389799952507019, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9699000120162964, 1.9697999954223633, 1.9697999954223633, 1.9697999954223633, 1.9696999788284302, 1.9696999788284302, 1.9696999788284302, 1.9696999788284302, 1.9696999788284302, 1.9696999788284302, 1.9696999788284302, 1.969599962234497, 1.969599962234497, 1.969599962234497, 1.969599962234497, 1.969599962234497, 1.969599962234497, 1.969499945640564, 1.969499945640564, 1.969499945640564, 1.969499945640564, 1.9694000482559204, 1.9694000482559204, 2.7509000301361084, 2.7509000301361084, 2.7509000301361084, 2.7509000301361084, 2.7509000301361084, 2.7507998943328857, 2.7507998943328857, 2.7507998943328857, 2.7507998943328857, 2.750699996948242, 2.750699996948242, 2.7506000995635986, 2.750499963760376, 2.750499963760376, 2.750499963760376, 2.7504000663757324, 2.7504000663757324, 2.7502999305725098, 2.7502999305725098, 2.750200033187866, 2.750200033187866, 2.750200033187866, 2.7500998973846436, 2.7500998973846436, 2.7499001026153564, 2.749799966812134, 2.749799966812134, 2.749799966812134, 2.7497000694274902, 2.7497000694274902, 2.7385001182556152, 2.791300058364868, 2.791300058364868, 2.791300058364868, 2.7911999225616455, 2.7911999225616455, 2.7911999225616455, 2.7911999225616455, 2.7911999225616455, 2.7911999225616455, 2.791100025177002, 2.791100025177002, 2.791100025177002, 2.791100025177002, 2.791100025177002, 2.7909998893737793, 2.7909998893737793, 2.7908999919891357, 2.7908999919891357, 2.7908999919891357, 2.7906999588012695, 2.7906999588012695, 2.790600061416626, 2.790600061416626, 2.7904999256134033, 2.7904000282287598, 2.7904000282287598, 2.790299892425537, 2.790299892425537, 2.790299892425537, 2.790299892425537, 3.0590999126434326, 3.0590999126434326, 3.059000015258789, 3.059000015258789, 3.059000015258789, 3.0589001178741455, 3.0589001178741455, 3.0589001178741455, 3.0589001178741455, 3.0589001178741455, 3.0589001178741455, 3.058799982070923, 3.0587000846862793, 3.0585999488830566, 3.0585999488830566, 3.0585999488830566, 3.058500051498413, 3.058500051498413, 3.0583999156951904, 3.058300018310547, 3.058300018310547, 3.058199882507324, 3.058199882507324, 3.058000087738037, 3.058000087738037, 3.0578999519348145, 3.0578999519348145, 3.0578999519348145, 3.0578999519348145, 3.057800054550171, 3.06030011177063, 3.0601999759674072, 3.0601000785827637, 3.0601000785827637, 3.0601000785827637, 3.059999942779541, 3.059999942779541, 3.059999942779541, 3.059999942779541, 3.0597000122070312, 3.0597000122070312, 3.0596001148223877, 3.0596001148223877, 3.059299945831299, 3.0592000484466553, 3.0592000484466553, 3.0590999126434326, 3.0590999126434326, 3.0590999126434326, 3.059000015258789, 3.0589001178741455, 3.058799982070923, 3.058799982070923, 3.0587000846862793, 3.0585999488830566, 3.0585999488830566, 3.0583999156951904, 3.0583999156951904, 3.058300018310547, 3.0580999851226807, 3.31469988822937, 3.31469988822937, 3.3145999908447266, 3.3145999908447266, 3.314500093460083, 3.314500093460083, 3.3143999576568604, 3.314300060272217, 3.314300060272217, 3.314300060272217, 3.314199924468994, 3.3141000270843506, 3.3141000270843506, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.313999891281128, 3.3138999938964844, 3.3138999938964844, 3.3138999938964844, 3.313800096511841, 3.313800096511841, 3.313800096511841, 3.313800096511841, 3.313800096511841, 3.313699960708618, 3.4203999042510986, 3.4203999042510986, 3.420300006866455, 3.4202001094818115, 3.4202001094818115, 3.4202001094818115, 3.420099973678589, 3.420099973678589, 3.420099973678589, 3.420099973678589, 3.4200000762939453, 3.4198999404907227, 3.4198999404907227, 3.4198999404907227, 3.4198999404907227, 3.419800043106079, 3.419800043106079, 3.419800043106079, 3.4196999073028564, 3.419600009918213, 3.419600009918213, 3.4195001125335693, 3.4195001125335693, 3.4193999767303467, 3.4193999767303467, 3.419300079345703, 3.419300079345703, 3.419300079345703, 3.419300079345703, 3.4191999435424805, 3.4223999977111816, 3.422300100326538, 3.4221999645233154, 3.4221999645233154, 3.421999931335449, 3.4219000339508057, 3.421799898147583, 3.421799898147583, 3.4217000007629395, 3.4217000007629395, 3.4217000007629395, 3.421600103378296, 3.421600103378296, 3.421600103378296, 3.4214999675750732, 3.421299934387207, 3.4212000370025635, 3.4212000370025635, 3.4212000370025635, 3.4212000370025635, 3.421099901199341, 3.421099901199341, 3.4210000038146973, 3.4210000038146973, 3.4209001064300537, 3.420799970626831, 3.4207000732421875, 3.420599937438965, 3.4205000400543213, 3.4205000400543213, 3.4651999473571777, 3.4649999141693115, 3.4649999141693115, 3.4649999141693115, 3.4647998809814453, 3.4646999835968018, 3.4646999835968018, 3.4646999835968018, 3.4646999835968018, 3.464600086212158, 3.464600086212158, 3.4644999504089355, 3.4644999504089355, 3.464400053024292, 3.464400053024292, 3.464400053024292, 3.464400053024292, 3.4642999172210693, 3.4642999172210693, 3.4642999172210693, 3.4642999172210693, 3.464200019836426, 3.464200019836426, 3.464099884033203, 3.464099884033203, 3.464099884033203, 3.464099884033203, 3.4639999866485596, 3.4639999866485596, 3.4639999866485596, 3.5299999713897705, 3.5299999713897705, 3.5299999713897705, 3.5299999713897705, 3.529900074005127, 3.5297999382019043, 3.5297999382019043, 3.5297999382019043, 3.5297000408172607, 3.529599905014038, 3.529599905014038, 3.5295000076293945, 3.5295000076293945, 3.5295000076293945, 3.5292000770568848, 3.5292000770568848, 3.5290000438690186, 3.528899908065796, 3.5288000106811523, 3.5288000106811523, 3.528700113296509, 3.528599977493286, 3.528599977493286, 3.528599977493286, 3.5285000801086426, 3.5285000801086426, 3.5285000801086426, 3.5283000469207764, 3.5281999111175537, 3.52810001373291, 3.530900001525879, 3.530900001525879, 3.530900001525879, 3.5306999683380127, 3.5306999683380127, 3.5306999683380127, 3.530600070953369, 3.530600070953369, 3.5304999351501465, 3.5304999351501465, 3.5302999019622803, 3.5302000045776367, 3.5299999713897705, 3.5299999713897705, 3.5297999382019043, 3.5297000408172607, 3.5297000408172607, 3.5297000408172607, 3.529599905014038, 3.5295000076293945, 3.529400110244751, 3.5292000770568848, 3.529099941253662, 3.529099941253662, 3.529099941253662, 3.529099941253662, 3.528899908065796, 3.5288000106811523, 3.5288000106811523, 3.528700113296509, 3.548099994659424, 3.5480000972747803, 3.5480000972747803, 3.5480000972747803, 3.547800064086914, 3.547800064086914, 3.547800064086914, 3.5476999282836914, 3.547600030899048, 3.547600030899048, 3.547499895095825, 3.547499895095825, 3.5473999977111816, 3.5473999977111816, 3.547300100326538, 3.547300100326538, 3.547300100326538, 3.5471999645233154, 3.5471999645233154, 3.5471999645233154, 3.547100067138672, 3.546999931335449, 3.5469000339508057, 3.546799898147583, 3.546799898147583, 3.5467000007629395, 3.5467000007629395, 3.546600103378296, 3.546600103378296, 3.5464999675750732, 3.585099935531616, 3.585099935531616, 3.5850000381469727, 3.58489990234375, 3.5848000049591064, 3.584700107574463, 3.5845999717712402, 3.584399938583374, 3.5843000411987305, 3.5843000411987305, 3.584199905395508, 3.5841000080108643, 3.5841000080108643, 3.5840001106262207, 3.5840001106262207, 3.5840001106262207, 3.583899974822998, 3.583899974822998, 3.5838000774383545, 3.5838000774383545, 3.5838000774383545, 3.583699941635132, 3.583699941635132, 3.583699941635132, 3.5836000442504883, 3.5836000442504883, 3.5836000442504883, 3.5834999084472656, 3.5833001136779785, 3.5833001136779785, 3.621999979019165, 3.621999979019165, 3.621999979019165, 3.621999979019165, 3.6217000484466553, 3.6215999126434326, 3.621500015258789, 3.621500015258789, 3.6214001178741455, 3.6214001178741455, 3.621299982070923, 3.6212000846862793, 3.6210999488830566, 3.6210999488830566, 3.6210999488830566, 3.621000051498413, 3.6208999156951904, 3.6208999156951904, 3.620699882507324, 3.620699882507324, 3.6205999851226807, 3.620500087738037, 3.620500087738037, 3.620500087738037, 3.6203999519348145, 3.6203999519348145, 3.6203999519348145, 3.620300054550171, 3.620300054550171, 3.6201999187469482, 3.624300003051758, 3.6242001056671143, 3.6240999698638916, 3.624000072479248, 3.624000072479248, 3.624000072479248, 3.624000072479248, 3.624000072479248, 3.6238999366760254, 3.623800039291382, 3.623699903488159, 3.623199939727783, 3.6231000423431396, 3.6231000423431396, 3.622999906539917, 3.622999906539917, 3.622999906539917, 3.6229000091552734, 3.6229000091552734, 3.6226999759674072, 3.622499942779541, 3.622499942779541, 3.622499942779541, 3.6224000453948975, 3.6224000453948975, 3.6224000453948975, 3.622299909591675, 3.6222000122070312, 3.6221001148223877, 3.6219000816345215, 3.683000087738037, 3.683000087738037, 3.6828999519348145, 3.682800054550171, 3.6826999187469482, 3.6826999187469482, 3.6826999187469482, 3.682499885559082, 3.682499885559082, 3.682300090789795, 3.682300090789795, 3.6821999549865723, 3.6821000576019287, 3.6819000244140625, 3.6819000244140625, 3.68179988861084, 3.68179988861084, 3.68179988861084, 3.6816999912261963, 3.6816000938415527, 3.6816000938415527, 3.6816000938415527, 3.68149995803833, 3.68149995803833, 3.68149995803833, 3.6814000606536865, 3.6814000606536865, 3.681299924850464, 3.6812000274658203, 3.6812000274658203, 3.7600998878479004, 3.759999990463257, 3.7599000930786133, 3.759700059890747, 3.759700059890747, 3.759700059890747, 3.7595999240875244, 3.759500026702881, 3.759500026702881, 3.759399890899658, 3.7592999935150146, 3.759200096130371, 3.7590999603271484, 3.7590999603271484, 3.7590999603271484, 3.759000062942505, 3.759000062942505, 3.759000062942505, 3.7588000297546387, 3.7588000297546387, 3.7583000659942627, 3.75819993019104, 3.7581000328063965, 3.7578999996185303, 3.7578999996185303, 3.757699966430664, 3.757699966430664, 3.757699966430664, 3.757699966430664, 3.7574000358581543, 3.7892000675201416, 3.789099931716919, 3.7890000343322754, 3.7890000343322754, 3.7890000343322754, 3.7890000343322754, 3.7890000343322754, 3.7890000343322754, 3.788800001144409, 3.7887001037597656, 3.7887001037597656, 3.788599967956543, 3.788300037384033, 3.7881999015808105, 3.7881999015808105, 3.7880001068115234, 3.787899971008301, 3.7878000736236572, 3.7876999378204346, 3.7874999046325684, 3.7874999046325684, 3.7873001098632812, 3.7873001098632812, 3.7871999740600586, 3.7871999740600586, 3.7871999740600586, 3.7871999740600586, 3.787100076675415, 3.7869999408721924, 3.786900043487549, 3.911099910736084, 3.910900115966797, 3.910900115966797, 3.910900115966797, 3.910900115966797, 3.910799980163574, 3.910799980163574, 3.910799980163574, 3.9105000495910645, 3.910399913787842, 3.9103000164031982, 3.9102001190185547, 3.910099983215332, 3.909899950027466, 3.909899950027466, 3.909899950027466, 3.9096999168395996, 3.9096999168395996, 3.909600019454956, 3.9093000888824463, 3.9093000888824463, 3.9091999530792236, 3.9089999198913574, 3.9089999198913574, 3.908900022506714, 3.908900022506714, 3.908900022506714, 3.908799886703491, 3.908600091934204, 3.9084999561309814], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.7906999588012695, -1.8696000576019287, -2.6977999210357666, -2.809499979019165, -3.052299976348877, -3.070199966430664, -3.4279000759124756, -3.4267001152038574, -3.462899923324585, -3.8254001140594482, -4.000699996948242, -4.005099773406982, -4.068600177764893, -4.14139986038208, -4.200900077819824, -4.282400131225586, -4.311100006103516, -4.440999984741211, -4.4629998207092285, -4.471199989318848, -4.577400207519531, -4.752600193023682, -4.765200138092041, -4.781300067901611, -4.827000141143799, -4.901700019836426, -4.97790002822876, -5.211900234222412, -5.212800025939941, -5.252999782562256, -1.6394000053405762, -2.0378000736236572, -2.3578999042510986, -2.864500045776367, -2.915299892425537, -2.9481000900268555, -3.2881999015808105, -3.539599895477295, -3.807499885559082, -4.043000221252441, -4.375699996948242, -4.520100116729736, -4.533199787139893, -4.652100086212158, -4.66510009765625, -4.684199810028076, -4.857600212097168, -4.912799835205078, -5.058599948883057, -5.071499824523926, -5.094799995422363, -5.099699974060059, -5.132599830627441, -5.139100074768066, -5.262800216674805, -5.2789998054504395, -5.28410005569458, -5.371500015258789, -5.573299884796143, -5.599299907684326, -1.8428000211715698, -2.2351999282836914, -2.759399890899658, -2.8052000999450684, -3.061199903488159, -3.0845999717712402, -3.2005999088287354, -3.530400037765503, -3.5827999114990234, -4.052800178527832, -4.064599990844727, -4.302700042724609, -4.506400108337402, -4.513599872589111, -4.59630012512207, -4.704699993133545, -4.846399784088135, -4.932799816131592, -4.9390997886657715, -5.084099769592285, -5.089300155639648, -5.104499816894531, -5.23330020904541, -5.241399765014648, -5.448200225830078, -5.484300136566162, -5.487100124359131, -5.489200115203857, -5.561999797821045, -5.60129976272583, -4.2006001472473145, -1.9215999841690063, -2.5018999576568604, -2.6370999813079834, -3.0181000232696533, -3.0501999855041504, -3.0871999263763428, -3.1640000343322754, -3.503700017929077, -3.695199966430664, -3.705899953842163, -3.7451999187469482, -3.888700008392334, -4.038700103759766, -4.0802001953125, -4.148900032043457, -4.248600006103516, -4.446000099182129, -4.50439977645874, -4.509099960327148, -4.856599807739258, -4.88730001449585, -5.064700126647949, -5.07480001449585, -5.168399810791016, -5.242199897766113, -5.250400066375732, -5.343599796295166, -5.35830020904541, -5.359600067138672, -5.375999927520752, -1.7072999477386475, -2.681299924850464, -2.9642999172210693, -3.1556999683380127, -3.194499969482422, -3.4616000652313232, -3.569200038909912, -3.723099946975708, -3.7451999187469482, -3.811199903488159, -3.821899890899658, -3.861599922180176, -4.138400077819824, -4.363800048828125, -4.367800235748291, -4.442399978637695, -4.586400032043457, -4.613999843597412, -4.804900169372559, -4.931099891662598, -4.933199882507324, -4.96150016784668, -5.01800012588501, -5.1545000076293945, -5.226600170135498, -5.246699810028076, -5.259500026702881, -5.259699821472168, -5.273499965667725, -5.368800163269043, -1.0056999921798706, -2.4793999195098877, -3.507699966430664, -3.511199951171875, -3.6540000438690186, -3.930000066757202, -3.9481000900268555, -3.984600067138672, -4.038400173187256, -4.610400199890137, -4.626100063323975, -4.692200183868408, -4.714300155639648, -5.034800052642822, -5.154200077056885, -5.156300067901611, -5.232100009918213, -5.256199836730957, -5.266499996185303, -5.32420015335083, -5.395699977874756, -5.454899787902832, -5.470300197601318, -5.516600131988525, -5.5731000900268555, -5.611999988555908, -5.703400135040283, -5.70389986038208, -5.763700008392334, -5.830100059509277, -2.2337000370025635, -2.7597999572753906, -3.259500026702881, -3.3433001041412354, -3.562999963760376, -3.5752999782562256, -3.786099910736084, -4.020699977874756, -4.047599792480469, -4.098100185394287, -4.214399814605713, -4.425600051879883, -4.426400184631348, -4.503300189971924, -4.512499809265137, -4.516499996185303, -4.521299839019775, -4.583700180053711, -4.6143999099731445, -4.617300033569336, -4.623899936676025, -4.6890997886657715, -4.7204999923706055, -4.724899768829346, -4.745500087738037, -4.752500057220459, -4.754799842834473, -4.760300159454346, -4.760300159454346, -4.85699987411499, -2.6851999759674072, -3.0304999351501465, -3.325000047683716, -3.559499979019165, -3.594099998474121, -3.6266000270843506, -3.839200019836426, -3.956899881362915, -3.982599973678589, -3.9925999641418457, -4.131700038909912, -4.212600231170654, -4.236700057983398, -4.3043999671936035, -4.316999912261963, -4.366300106048584, -4.373000144958496, -4.432199954986572, -4.568900108337402, -4.644700050354004, -4.679800033569336, -4.744699954986572, -4.753499984741211, -4.815800189971924, -4.8709001541137695, -4.880199909210205, -4.885000228881836, -4.886600017547607, -4.923099994659424, -4.974999904632568, -1.8968000411987305, -2.430000066757202, -3.254499912261963, -3.3901000022888184, -3.9017999172210693, -4.044000148773193, -4.171199798583984, -4.243299961090088, -4.306700229644775, -4.356400012969971, -4.384799957275391, -4.443399906158447, -4.445400238037109, -4.5345001220703125, -4.581399917602539, -4.773200035095215, -4.855100154876709, -4.896900177001953, -4.905200004577637, -4.924799919128418, -4.964399814605713, -4.986999988555908, -5.045899868011475, -5.047999858856201, -5.090700149536133, -5.149499893188477, -5.202000141143799, -5.296899795532227, -5.313799858093262, -5.328499794006348, -2.145699977874756, -3.2409000396728516, -3.3106000423431396, -3.3392999172210693, -3.861799955368042, -4.026299953460693, -4.158400058746338, -4.161499977111816, -4.1743998527526855, -4.3256001472473145, -4.327300071716309, -4.343200206756592, -4.3719000816345215, -4.4664998054504395, -4.471199989318848, -4.493500232696533, -4.523399829864502, -4.599599838256836, -4.617099761962891, -4.646399974822998, -4.650300025939941, -4.714600086212158, -4.72599983215332, -4.785900115966797, -4.794000148773193, -4.811699867248535, -4.817699909210205, -4.86329984664917, -4.910299777984619, -4.926599979400635, -2.388700008392334, -2.4693000316619873, -2.504199981689453, -2.6338000297546387, -3.1709001064300537, -3.305299997329712, -3.3910999298095703, -3.5527000427246094, -3.6633999347686768, -3.914400100708008, -3.9618000984191895, -4.0507001876831055, -4.127099990844727, -4.160699844360352, -4.521500110626221, -4.534999847412109, -4.7357001304626465, -4.808800220489502, -4.863699913024902, -4.888800144195557, -4.94920015335083, -4.971399784088135, -5.000899791717529, -5.026899814605713, -5.0391998291015625, -5.065100193023682, -5.085000038146973, -5.1682000160217285, -5.240900039672852, -5.283199787139893, -1.9474999904632568, -2.1953999996185303, -2.6080000400543213, -3.2569000720977783, -3.4848999977111816, -3.5369999408721924, -3.5908000469207764, -3.5917000770568848, -3.824199914932251, -3.983599901199341, -4.196700096130371, -4.325799942016602, -4.579699993133545, -4.615799903869629, -4.740200042724609, -4.833799839019775, -4.834499835968018, -4.858099937438965, -4.905900001525879, -4.97130012512207, -5.071100234985352, -5.188300132751465, -5.199999809265137, -5.215099811553955, -5.231400012969971, -5.231900215148926, -5.31220006942749, -5.347700119018555, -5.364500045776367, -5.404900074005127, -2.281599998474121, -2.922100067138672, -2.944000005722046, -3.1998000144958496, -3.6324000358581543, -3.6370999813079834, -3.808000087738037, -3.960400104522705, -4.0680999755859375, -4.111400127410889, -4.234000205993652, -4.264599800109863, -4.375699996948242, -4.381800174713135, -4.469399929046631, -4.515100002288818, -4.547800064086914, -4.6072001457214355, -4.633200168609619, -4.63730001449585, -4.668099880218506, -4.792500019073486, -4.871099948883057, -4.903900146484375, -4.929900169372559, -4.979800224304199, -5.021900177001953, -5.0289998054504395, -5.080999851226807, -5.103799819946289, -2.297800064086914, -2.591900110244751, -2.8952999114990234, -3.415600061416626, -3.6691999435424805, -3.919600009918213, -4.022500038146973, -4.325900077819824, -4.461100101470947, -4.491099834442139, -4.576099872589111, -4.617499828338623, -4.632900238037109, -4.7204999923706055, -4.720699787139893, -4.750100135803223, -4.79040002822876, -4.847400188446045, -4.860000133514404, -4.896599769592285, -4.906700134277344, -4.926300048828125, -4.94920015335083, -4.97599983215332, -4.985300064086914, -5.041900157928467, -5.043099880218506, -5.090700149536133, -5.1616997718811035, -5.192999839782715, -2.297600030899048, -2.423799991607666, -2.4842000007629395, -2.8857998847961426, -3.7553000450134277, -3.9788999557495117, -4.096399784088135, -4.117800235748291, -4.167799949645996, -4.17609977722168, -4.304599761962891, -4.400300025939941, -4.530399799346924, -4.549600124359131, -4.571400165557861, -4.609099864959717, -4.681600093841553, -4.6921000480651855, -4.863999843597412, -4.867599964141846, -4.945700168609619, -4.986299991607666, -4.989999771118164, -4.992000102996826, -5.031300067901611, -5.042300224304199, -5.074100017547607, -5.108500003814697, -5.11460018157959, -5.140600204467773, -1.7697999477386475, -2.7258999347686768, -2.996799945831299, -3.3303000926971436, -3.4031999111175537, -3.407599925994873, -3.442699909210205, -3.450200080871582, -3.718100070953369, -3.792099952697754, -3.9872000217437744, -4.61929988861084, -4.6880998611450195, -4.719299793243408, -4.738399982452393, -4.76039981842041, -4.762599945068359, -4.83459997177124, -4.878600120544434, -4.975299835205078, -5.076000213623047, -5.0914998054504395, -5.113800048828125, -5.125199794769287, -5.130300045013428, -5.168900012969971, -5.2164998054504395, -5.264599800109863, -5.281899929046631, -5.37529993057251, -2.379300117492676, -2.3864998817443848, -2.8882999420166016, -3.501300096511841, -3.607300043106079, -3.6542000770568848, -3.6907999515533447, -4.002099990844727, -4.082399845123291, -4.295499801635742, -4.306099891662598, -4.344600200653076, -4.523600101470947, -4.624100208282471, -4.65339994430542, -4.700900077819824, -4.723199844360352, -4.762700080871582, -4.797699928283691, -4.890399932861328, -4.899700164794922, -4.9054999351501465, -4.906799793243408, -4.918600082397461, -4.94950008392334, -4.9745001792907715, -4.998899936676025, -5.053500175476074, -5.102099895477295, -5.127500057220459, -2.2772998809814453, -2.8217999935150146, -3.248500108718872, -3.5439000129699707, -3.560699939727783, -3.6493000984191895, -3.831199884414673, -3.8952999114990234, -3.964400053024292, -4.128399848937988, -4.198500156402588, -4.341300010681152, -4.397200107574463, -4.409200191497803, -4.434299945831299, -4.508999824523926, -4.52400016784668, -4.52810001373291, -4.678199768066406, -4.684000015258789, -5.004799842834473, -5.0279998779296875, -5.089600086212158, -5.18720006942749, -5.190199851989746, -5.2519001960754395, -5.26140022277832, -5.271500110626221, -5.271999835968018, -5.367599964141846, -2.8178000450134277, -3.015899896621704, -3.253000020980835, -3.2776999473571777, -3.303999900817871, -3.4168999195098877, -3.4226999282836914, -3.4686999320983887, -3.7901999950408936, -3.869499921798706, -3.9321999549865723, -4.0980000495910645, -4.407700061798096, -4.467700004577637, -4.505499839782715, -4.65369987487793, -4.708099842071533, -4.779099941253662, -4.821199893951416, -4.942200183868408, -4.9695000648498535, -5.050000190734863, -5.053599834442139, -5.103499889373779, -5.107500076293945, -5.107600212097168, -5.117700099945068, -5.124800205230713, -5.184100151062012, -5.247499942779541, -2.7392001152038574, -3.2734999656677246, -3.27810001373291, -3.3120999336242676, -3.388700008392334, -3.46560001373291, -3.5481998920440674, -3.5576000213623047, -3.907099962234497, -4.047599792480469, -4.184899806976318, -4.310699939727783, -4.3368000984191895, -4.505499839782715, -4.5258002281188965, -4.560299873352051, -4.645699977874756, -4.661499977111816, -4.756100177764893, -4.8927998542785645, -4.903800010681152, -4.940100193023682, -5.03249979019165, -5.042600154876709, -5.060200214385986, -5.066100120544434, -5.06790018081665, -5.10260009765625, -5.196199893951416, -5.2153000831604]}, \"token.table\": {\"Topic\": [8, 7, 8, 20, 9, 8, 17, 4, 3, 9, 9, 8, 3, 5, 11, 12, 20, 4, 2, 20, 10, 13, 7, 12, 3, 2, 16, 6, 9, 7, 19, 3, 1, 13, 18, 14, 7, 3, 20, 16, 5, 10, 6, 3, 1, 16, 2, 4, 15, 14, 19, 4, 19, 18, 13, 5, 5, 19, 10, 5, 20, 19, 17, 10, 7, 10, 3, 6, 1, 7, 11, 13, 9, 8, 16, 13, 13, 14, 12, 16, 4, 12, 11, 15, 20, 18, 20, 9, 17, 11, 12, 7, 16, 14, 3, 14, 15, 2, 16, 19, 16, 20, 10, 15, 6, 12, 16, 19, 3, 14, 14, 9, 12, 20, 13, 20, 12, 18, 17, 11, 9, 14, 8, 4, 2, 15, 20, 6, 1, 14, 17, 18, 9, 1, 13, 13, 7, 17, 7, 13, 15, 6, 16, 12, 11, 1, 13, 10, 8, 14, 19, 4, 14, 19, 12, 8, 11, 18, 18, 17, 11, 16, 19, 20, 15, 20, 20, 10, 18, 9, 18, 4, 17, 18, 3, 5, 7, 9, 12, 11, 15, 14, 5, 3, 11, 3, 12, 16, 18, 3, 3, 20, 11, 6, 16, 10, 20, 10, 12, 3, 12, 2, 2, 11, 12, 11, 5, 9, 12, 17, 8, 12, 17, 3, 17, 10, 12, 8, 9, 4, 10, 1, 15, 19, 11, 17, 4, 2, 10, 15, 5, 7, 3, 7, 11, 3, 19, 20, 14, 10, 4, 7, 6, 4, 1, 16, 19, 7, 1, 16, 5, 16, 7, 19, 6, 17, 1, 9, 9, 4, 19, 1, 20, 8, 14, 4, 7, 18, 9, 13, 12, 1, 6, 13, 12, 18, 10, 13, 6, 17, 15, 5, 12, 14, 9, 4, 20, 10, 19, 8, 10, 9, 3, 17, 14, 8, 2, 13, 18, 8, 5, 16, 16, 15, 11, 6, 7, 15, 13, 18, 14, 18, 20, 15, 15, 20, 8, 18, 18, 6, 17, 4, 7, 18, 15, 19, 20, 4, 10, 9, 20, 2, 9, 3, 3, 17, 14, 13, 14, 6, 16, 5, 20, 5, 1, 11, 12, 1, 13, 12, 1, 8, 17, 10, 2, 3, 11, 15, 6, 11, 4, 16, 1, 11, 5, 11, 16, 10, 20, 7, 6, 15, 1, 10, 13, 19, 6, 15, 6, 16, 5, 2, 14, 3, 9, 14, 16, 14, 14, 5, 8, 15, 10, 16, 5, 7, 14, 12, 19, 8, 5, 11, 15, 11, 5, 13, 5, 5, 8, 18, 17, 17, 12, 16, 8, 15, 14, 19, 13, 12, 11, 17, 3, 10, 18, 1, 17, 13, 17, 15, 2, 6, 13, 11, 2, 17, 7, 20, 2, 7, 16, 10, 8, 15, 4, 6, 10, 2, 15, 18, 18, 18, 19, 13, 8, 6, 17, 20, 18, 6, 12, 1, 7, 12, 10, 6, 6, 14, 14, 6, 15, 10, 16, 17, 13, 8, 19, 9, 9, 5, 11, 9, 9, 7, 4, 2, 1, 1, 14, 11, 5, 4, 19, 1, 2, 8, 10, 13, 12, 13, 16, 10, 15, 2, 6, 17, 14, 3, 4, 3, 7, 19, 4, 18, 8, 5, 3, 16, 7, 20, 4, 19, 16, 1, 18, 15, 1, 13, 19, 19, 2, 2, 4, 9, 15, 1, 13, 3, 13, 10, 5, 9, 2, 5, 18, 2, 2, 1, 20, 11, 17, 9, 7, 9, 1, 1, 8, 2, 4, 18, 3, 17, 5, 1, 14, 15, 12, 2, 19, 19, 9, 8, 2, 11, 8, 8, 6, 5, 4, 8, 11, 20, 4, 7, 7, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 4, 17, 2, 6, 6, 1, 19, 4, 7, 6, 5, 3, 18], \"Freq\": [0.9992133378982544, 0.9999271035194397, 0.9993935227394104, 0.9980887770652771, 0.999379575252533, 0.9995500445365906, 0.9982234835624695, 0.9998323917388916, 0.9997479319572449, 0.9993088245391846, 0.9993384480476379, 0.999031662940979, 0.9998983144760132, 0.9989125728607178, 0.9990913271903992, 0.9976719617843628, 0.9959539175033569, 0.9994568228721619, 0.9998252987861633, 0.9996577501296997, 0.999654233455658, 0.9995184540748596, 0.9993718862533569, 0.9992858171463013, 0.9999240636825562, 0.9999775886535645, 0.9975270628929138, 0.9995719790458679, 0.9987877011299133, 0.9997223615646362, 0.9980652332305908, 0.9990435838699341, 0.9999693036079407, 0.9993489980697632, 0.9979010224342346, 0.9998413920402527, 0.9988968372344971, 0.9999759197235107, 0.9992321729660034, 0.9990244507789612, 0.999610424041748, 0.9989777207374573, 0.9989029169082642, 0.9982460737228394, 0.9999309778213501, 0.9982454776763916, 0.9997658729553223, 0.9997168183326721, 0.9998308420181274, 0.9984191060066223, 0.9993554949760437, 0.9994413256645203, 0.9998388290405273, 0.9987210035324097, 0.9995961785316467, 0.9992324113845825, 0.9987591505050659, 0.9967210292816162, 0.9992019534111023, 0.9989041090011597, 0.999003529548645, 0.9982956647872925, 0.9969795346260071, 0.9993439316749573, 0.9994778037071228, 0.9985803961753845, 0.9987968802452087, 0.9993664622306824, 0.9999266266822815, 0.9993473291397095, 0.9998489618301392, 0.9997336864471436, 0.9984518885612488, 0.9989198446273804, 0.9999387264251709, 0.9986680150032043, 0.9985706210136414, 0.9989672303199768, 0.9995264410972595, 0.9988816976547241, 0.999537467956543, 0.9999338388442993, 0.9987485408782959, 0.9988882541656494, 0.9979119300842285, 0.9999319911003113, 0.9983512759208679, 0.9983408451080322, 0.9990008473396301, 0.9991451501846313, 0.9999507665634155, 0.998952329158783, 0.9979183077812195, 0.9991600513458252, 0.9998136758804321, 0.998857319355011, 0.9976991415023804, 0.999975860118866, 0.9979701042175293, 0.9974421858787537, 0.9984968304634094, 0.998239278793335, 0.9987092018127441, 0.9982367157936096, 0.997154176235199, 0.9995941519737244, 0.9979049563407898, 0.9991053342819214, 0.9996631145477295, 0.9985508322715759, 0.9991466403007507, 0.9982024431228638, 0.9987769722938538, 0.997845470905304, 0.999492347240448, 0.9981479644775391, 0.9978552460670471, 0.9973766207695007, 0.9999077320098877, 0.9982712864875793, 0.9982228875160217, 0.9988306760787964, 0.9995079636573792, 0.9997071027755737, 0.9997642636299133, 0.9979225993156433, 0.998780369758606, 0.9980176091194153, 0.9998648762702942, 0.9995555877685547, 0.999315083026886, 0.9992779493331909, 0.9993085861206055, 0.999851644039154, 0.9990296363830566, 0.9986743927001953, 0.9996567368507385, 0.9995126724243164, 0.998958945274353, 0.9994266033172607, 0.9985955357551575, 0.9979277849197388, 0.9969721436500549, 0.998484194278717, 0.9977301359176636, 0.9999834895133972, 0.9994542002677917, 0.9991862177848816, 0.9992727041244507, 0.9985076785087585, 0.998547375202179, 0.9997968077659607, 0.9992357492446899, 0.9996777772903442, 0.999552845954895, 0.99858158826828, 0.9976581335067749, 0.9986563920974731, 0.9989379644393921, 0.9992777109146118, 0.997552752494812, 0.9991371035575867, 0.9977517127990723, 0.9977548718452454, 0.9985775351524353, 0.9992371797561646, 0.9989988803863525, 0.9990657567977905, 0.9984685778617859, 0.998923659324646, 0.9972198605537415, 0.9996249079704285, 0.9983246922492981, 0.9989457726478577, 0.9996328353881836, 0.9986936450004578, 0.9996896386146545, 0.9972873330116272, 0.9982478022575378, 0.9993102550506592, 0.9993648529052734, 0.9981411695480347, 0.9986696839332581, 0.9999711513519287, 0.9989409446716309, 0.999383270740509, 0.997779130935669, 0.9995603561401367, 0.9991716146469116, 0.999903678894043, 0.9983507990837097, 0.9986772537231445, 0.9980355501174927, 0.9983533620834351, 0.999620258808136, 0.9998958110809326, 0.9995964765548706, 0.9991656541824341, 0.9999178647994995, 0.9989928603172302, 0.998346209526062, 0.9996362328529358, 0.9996637105941772, 0.999602735042572, 0.9993014931678772, 0.9993732571601868, 0.9996545314788818, 0.99944007396698, 0.9983881115913391, 0.9997692108154297, 0.99974524974823, 0.9988788366317749, 0.9987509250640869, 0.9991971850395203, 0.9994896650314331, 0.999190092086792, 0.9990277886390686, 0.9997989535331726, 0.9986276030540466, 0.9999442100524902, 0.9989715218544006, 0.999880313873291, 0.9989385008811951, 0.9991003274917603, 0.9993686079978943, 0.9984989166259766, 0.9995641112327576, 0.9994268417358398, 0.9998384118080139, 0.9994940757751465, 0.9998515844345093, 0.9990123510360718, 0.9997992515563965, 0.9992074370384216, 0.9982235431671143, 0.9995219707489014, 0.998126745223999, 0.9998160600662231, 0.9991557598114014, 0.9993478059768677, 0.9998213052749634, 0.9986029863357544, 0.9985033869743347, 0.999382495880127, 0.9998717308044434, 0.9978366494178772, 0.9985215067863464, 0.9989457726478577, 0.9999895691871643, 0.9981023669242859, 0.999179482460022, 0.9984918832778931, 0.9986610412597656, 0.9995086193084717, 0.999201774597168, 0.9985935091972351, 0.9999740719795227, 0.9983866810798645, 0.9978212714195251, 0.9992733597755432, 0.9993014931678772, 0.9999554753303528, 0.9995301365852356, 0.9996765851974487, 0.9983206987380981, 0.9998734593391418, 0.999679446220398, 0.999372661113739, 0.999687671661377, 0.9990394711494446, 0.998934268951416, 0.9999219179153442, 0.9989458918571472, 0.9997944831848145, 0.9974070191383362, 0.999140202999115, 0.998227059841156, 0.9986293315887451, 0.9988299012184143, 0.9985283613204956, 0.9987378120422363, 0.9992038011550903, 0.9996197819709778, 0.9992086291313171, 0.999518096446991, 0.9999275207519531, 0.9992393255233765, 0.9998726844787598, 0.9995341897010803, 0.9988957643508911, 0.9991582036018372, 0.999513566493988, 0.9996131658554077, 0.9997044205665588, 0.9993452429771423, 0.9995207190513611, 0.9997116327285767, 0.9986793994903564, 0.9964475035667419, 0.9995649456977844, 0.9987723231315613, 0.9997496008872986, 0.999513566493988, 0.9985448122024536, 0.9998087882995605, 0.9999738335609436, 0.999530017375946, 0.9983367323875427, 0.9998781681060791, 0.9997454285621643, 0.9990379810333252, 0.9985730051994324, 0.9971629977226257, 0.9981830716133118, 0.9989288449287415, 0.998530924320221, 0.9986501932144165, 0.9997931122779846, 0.9970167875289917, 0.9995698928833008, 0.9986357092857361, 0.9998140931129456, 0.998664140701294, 0.9998330473899841, 0.9979937076568604, 0.9992855191230774, 0.9982789158821106, 0.9997184872627258, 0.9993283152580261, 0.999041736125946, 0.9997739195823669, 0.9999232292175293, 0.9986942410469055, 0.9995073676109314, 0.9988155961036682, 0.9999157786369324, 0.9982008337974548, 0.9997179508209229, 0.9998345971107483, 0.9988151788711548, 0.9997631907463074, 0.9999465942382812, 0.9996744394302368, 0.999817967414856, 0.9999141693115234, 0.9997013211250305, 0.9990027546882629, 0.9998099207878113, 0.9990295171737671, 0.9993008971214294, 0.9999749660491943, 0.999581515789032, 0.9977439045906067, 0.9994906187057495, 0.9999457001686096, 0.9999211430549622, 0.9997416734695435, 0.9996843934059143, 0.9991732835769653, 0.9998969435691833, 0.9984061121940613, 0.9997190237045288, 0.9999312162399292, 0.9999001026153564, 0.9995606541633606, 0.9991521835327148, 0.9982710480690002, 0.9990234971046448, 0.9995431900024414, 0.9993105530738831, 0.9976889491081238, 0.9992358088493347, 0.9997844099998474, 0.9992386102676392, 0.9994061589241028, 0.9997697472572327, 0.9994992017745972, 0.9990521669387817, 0.9992079734802246, 0.9996160268783569, 0.9994799494743347, 0.9994590282440186, 0.9991000294685364, 0.9995943307876587, 0.9991948008537292, 0.9991500377655029, 0.9972701668739319, 0.9994178414344788, 0.9985886216163635, 0.9997047185897827, 0.999020516872406, 0.9988877773284912, 0.999222993850708, 0.9975393414497375, 0.9991932511329651, 0.9988241791725159, 0.9991751313209534, 0.9989454746246338, 0.9994457960128784, 0.999743640422821, 0.9987077713012695, 0.9990297555923462, 0.9986163973808289, 0.9999140501022339, 0.9996717572212219, 0.999312698841095, 0.9993682503700256, 0.9997021555900574, 0.9995152950286865, 0.9975150227546692, 0.9988816380500793, 0.9987505078315735, 0.9972869157791138, 0.9993007183074951, 0.9996944665908813, 0.9998806715011597, 0.9987444281578064, 0.997904360294342, 0.9991074800491333, 0.9995878338813782, 0.9996023178100586, 0.9990706443786621, 0.999478280544281, 0.9988003969192505, 0.9993314743041992, 0.9998030662536621, 0.9994033575057983, 0.9989882707595825, 0.9989855885505676, 0.9998577833175659, 0.9993180632591248, 0.9992989301681519, 0.9977939128875732, 0.9986413717269897, 0.9995158314704895, 0.9974077343940735, 0.9999205470085144, 0.9982263445854187, 0.9996293187141418, 0.999655544757843, 0.9992386102676392, 0.9989280700683594, 0.9986533522605896, 0.9991632103919983, 0.9998651146888733, 0.9984957575798035, 0.9985733032226562, 0.9996084570884705, 0.9993451833724976, 0.9995996356010437, 0.9992199540138245, 0.9987091422080994, 0.9973207712173462, 0.9987010359764099, 0.9987856149673462, 0.9987227916717529, 0.9978408217430115, 0.9985445141792297, 0.9993551969528198, 0.9995753765106201, 0.9988263845443726, 0.9998183250427246, 0.9998635053634644, 0.9994350075721741, 0.9986066222190857, 0.9986609816551208, 0.9988669157028198, 0.9981668591499329, 0.9992711544036865, 0.9983625411987305, 0.9987743496894836, 0.9994491338729858, 0.9996402263641357, 0.9984610080718994, 0.9992110729217529, 0.9997037649154663, 0.9982925653457642, 0.9996590614318848, 0.9993406534194946, 0.9997572302818298, 0.9996692538261414, 0.9998372197151184, 0.9999324083328247, 0.9987391233444214, 0.9994808435440063, 0.9995788335800171, 0.9999712109565735, 0.9999908208847046, 0.9998055696487427, 0.9997514486312866, 0.9998692870140076, 0.9998997449874878, 0.9997639656066895, 0.999873697757721, 0.999975860118866, 0.9994152188301086, 0.999434232711792, 0.9995554089546204, 0.9976504445075989, 0.9983740448951721, 0.9990100860595703, 0.9998508095741272, 0.9998052716255188, 0.9999848008155823, 0.9998798370361328, 0.9992203116416931, 0.9981589317321777, 0.9989287257194519, 0.9997028708457947, 0.9996089339256287, 0.9992877244949341, 0.9987078905105591, 0.9997920989990234, 0.9978301525115967, 0.9989147186279297, 0.9997779726982117, 0.9995021820068359, 0.9981507658958435, 0.9994645714759827, 0.9991651177406311, 0.9993665814399719, 0.9985746145248413, 0.9992278814315796, 0.9998641014099121, 0.9966993927955627, 0.9984607696533203, 0.9996718764305115, 0.9984775185585022, 0.9996677041053772, 0.9989195466041565, 0.9998530745506287, 0.9996174573898315, 0.998611330986023, 0.9980478286743164, 0.9992408156394958, 0.9998233914375305, 0.9979261159896851, 0.9993376135826111, 0.9994856119155884, 0.9991208910942078, 0.9998851418495178, 0.9985243082046509, 0.9999178647994995, 0.9990944862365723, 0.9983119964599609, 0.9998598694801331, 0.9997596740722656, 0.9999604821205139, 0.9990135431289673, 0.9982156753540039, 0.9994698166847229, 0.9982900023460388, 0.9997290372848511, 0.999815821647644, 0.9999105334281921, 0.9999260306358337, 0.9996205568313599, 0.9995048642158508, 0.9988346099853516, 0.9994791746139526, 0.9999172687530518, 0.9990578293800354, 0.9998907446861267, 0.9999093413352966, 0.9997827410697937, 0.9984407424926758, 0.9997356534004211, 0.9999533295631409, 0.9993001818656921, 0.9974746108055115, 0.9998656511306763, 0.9992774724960327, 0.9997693300247192, 0.9997548460960388, 0.9996191263198853, 0.9991452693939209, 0.9999263882637024, 0.9996856451034546, 0.9997260570526123, 0.9989957809448242, 0.9996925592422485, 0.9986476302146912, 0.9988567233085632, 0.9993380308151245, 0.9989917278289795, 0.9998477697372437, 0.9999210238456726, 0.0006737948278896511, 0.0010106922127306461, 0.9874463081359863, 0.00033689741394482553, 0.0006737948278896511, 0.0006737948278896511, 0.0010106922127306461, 0.0006737948278896511, 0.00033689741394482553, 0.0006737948278896511, 0.0006737948278896511, 0.00033689741394482553, 0.00033689741394482553, 0.00033689741394482553, 0.0006737948278896511, 0.0006737948278896511, 0.00033689741394482553, 0.0010106922127306461, 0.0016844869824126363, 0.00033689741394482553, 0.9999123215675354, 0.9982791543006897, 0.9997118711471558, 0.99969482421875, 0.9998621940612793, 0.9999894499778748, 0.9992080330848694, 0.9999745488166809, 0.9987112879753113, 0.9996695518493652, 0.9991776943206787, 0.9987967014312744, 0.9977865219116211], \"Term\": [\"abl\", \"about\", \"absolut\", \"accept\", \"act\", \"actual\", \"ad\", \"after\", \"again\", \"against\", \"ago\", \"agre\", \"all\", \"allow\", \"almost\", \"alon\", \"along\", \"alreadi\", \"also\", \"alway\", \"am\", \"amaz\", \"america\", \"american\", \"an\", \"and\", \"announc\", \"anoth\", \"anti\", \"any\", \"anymor\", \"anyth\", \"are\", \"around\", \"artist\", \"as\", \"ask\", \"at\", \"away\", \"awesom\", \"back\", \"band\", \"base\", \"bc\", \"be\", \"beat\", \"because\", \"been\", \"before\", \"behind\", \"being\", \"believ\", \"best\", \"bet\", \"better\", \"big\", \"biggest\", \"billion\", \"black\", \"blame\", \"blue\", \"board\", \"book\", \"both\", \"boy\", \"brand\", \"bring\", \"busi\", \"but\", \"buy\", \"by\", \"call\", \"came\", \"campaign\", \"can\", \"candid\", \"cannot\", \"car\", \"care\", \"catch\", \"caus\", \"chang\", \"check\", \"christma\", \"church\", \"cinema\", \"cinematographi\", \"citi\", \"claim\", \"clear\", \"climat\", \"close\", \"coach\", \"colleg\", \"come\", \"complet\", \"concern\", \"concert\", \"congress\", \"consid\", \"control\", \"cook\", \"correct\", \"corrupt\", \"cost\", \"could\", \"couldn\", \"count\", \"countri\", \"cours\", \"crash\", \"creat\", \"crime\", \"critic\", \"current\", \"custom\", \"cut\", \"cycl\", \"day\", \"deal\", \"definit\", \"dem\", \"democrat\", \"did\", \"didn\", \"die\", \"differ\", \"direct\", \"do\", \"does\", \"doesn\", \"doing\", \"domest\", \"don\", \"done\", \"dont\", \"down\", \"draft\", \"drop\", \"during\", \"each\", \"earli\", \"eat\", \"economi\", \"either\", \"elect\", \"end\", \"enjoy\", \"enough\", \"especi\", \"etc\", \"even\", \"event\", \"ever\", \"everi\", \"everyth\", \"exact\", \"excit\", \"expect\", \"face\", \"facebook\", \"fair\", \"fake\", \"fall\", \"famili\", \"fan\", \"far\", \"farmer\", \"favorit\", \"fear\", \"featur\", \"feel\", \"film\", \"final\", \"find\", \"fire\", \"first\", \"florida\", \"focus\", \"follow\", \"food\", \"fool\", \"footbal\", \"for\", \"forc\", \"forget\", \"fraud\", \"free\", \"friend\", \"from\", \"full\", \"fun\", \"fund\", \"futur\", \"game\", \"get\", \"give\", \"global\", \"go\", \"god\", \"gone\", \"gonna\", \"good\", \"got\", \"gotta\", \"govern\", \"great\", \"group\", \"gun\", \"guy\", \"had\", \"half\", \"hall\", \"hand\", \"happen\", \"happi\", \"hard\", \"has\", \"hate\", \"have\", \"having\", \"he\", \"head\", \"hear\", \"heard\", \"hell\", \"help\", \"her\", \"here\", \"high\", \"him\", \"hire\", \"his\", \"histori\", \"hold\", \"home\", \"honest\", \"hope\", \"hour\", \"hous\", \"how\", \"hungari\", \"idea\", \"idiot\", \"if\", \"illeg\", \"impact\", \"import\", \"in\", \"includ\", \"influenc\", \"instead\", \"interest\", \"into\", \"invest\", \"investig\", \"is\", \"isi\", \"islam\", \"isn\", \"issu\", \"it\", \"its\", \"job\", \"join\", \"just\", \"keep\", \"kid\", \"kill\", \"kind\", \"knew\", \"know\", \"la\", \"last\", \"late\", \"law\", \"lead\", \"leader\", \"learn\", \"least\", \"leav\", \"left\", \"let\", \"liber\", \"life\", \"like\", \"line\", \"live\", \"ll\", \"local\", \"london\", \"long\", \"look\", \"lose\", \"lost\", \"lot\", \"love\", \"low\", \"ma\", \"made\", \"major\", \"make\", \"man\", \"manag\", \"mani\", \"market\", \"marketplac\", \"matter\", \"me\", \"mean\", \"media\", \"meet\", \"melbourn\", \"member\", \"messag\", \"met\", \"mind\", \"miss\", \"moment\", \"money\", \"month\", \"more\", \"morn\", \"most\", \"mouth\", \"move\", \"mr\", \"much\", \"music\", \"muslim\", \"must\", \"my\", \"name\", \"nation\", \"nba\", \"need\", \"network\", \"never\", \"new\", \"news\", \"next\", \"nfl\", \"night\", \"no\", \"not\", \"noth\", \"novemb\", \"now\", \"number\", \"obama\", \"of\", \"off\", \"oh\", \"old\", \"on\", \"one\", \"only\", \"open\", \"opportun\", \"or\", \"organ\", \"other\", \"our\", \"out\", \"over\", \"own\", \"paid\", \"part\", \"parti\", \"pass\", \"past\", \"pay\", \"peopl\", \"perform\", \"person\", \"physic\", \"pick\", \"place\", \"plan\", \"play\", \"player\", \"pleas\", \"pm\", \"point\", \"polit\", \"poll\", \"pop\", \"post\", \"power\", \"presid\", \"presidenti\", \"pretti\", \"price\", \"primari\", \"probabl\", \"problem\", \"product\", \"protect\", \"protest\", \"put\", \"qb\", \"quit\", \"rais\", \"re\", \"read\", \"readi\", \"real\", \"realli\", \"record\", \"region\", \"remind\", \"report\", \"respons\", \"result\", \"rig\", \"right\", \"rock\", \"room\", \"rule\", \"run\", \"russian\", \"sale\", \"same\", \"saturday\", \"saw\", \"say\", \"school\", \"second\", \"secur\", \"see\", \"seen\", \"sell\", \"senat\", \"share\", \"she\", \"shoot\", \"should\", \"shouldn\", \"show\", \"sinc\", \"sing\", \"singl\", \"sir\", \"small\", \"so\", \"social_media\", \"sold\", \"some\", \"someon\", \"someth\", \"song\", \"soon\", \"sorri\", \"sound\", \"special\", \"spend\", \"sport\", \"st\", \"stand\", \"start\", \"stay\", \"still\", \"stock\", \"stop\", \"stori\", \"strategi\", \"street\", \"student\", \"stupid\", \"success\", \"suck\", \"sunday\", \"support\", \"suppos\", \"sure\", \"take\", \"talent\", \"talk\", \"target\", \"team\", \"tell\", \"terror\", \"terrorist\", \"th\", \"than\", \"thank\", \"that\", \"the\", \"their\", \"them\", \"there\", \"they\", \"thing\", \"think\", \"this\", \"those\", \"though\", \"thought\", \"threat\", \"through\", \"throw\", \"ticket\", \"time\", \"to\", \"today\", \"togeth\", \"told\", \"tomorrow\", \"tonight\", \"too\", \"top\", \"total\", \"tri\", \"trip\", \"true\", \"trump\", \"turn\", \"tv\", \"tweet\", \"twitter\", \"two\", \"uk\", \"until\", \"up\", \"upcom\", \"ur\", \"us\", \"usa\", \"use\", \"valu\", \"ve\", \"very\", \"video\", \"view\", \"voic\", \"vote\", \"voter\", \"wait\", \"walk\", \"wanna\", \"want\", \"war\", \"was\", \"wasn\", \"wast\", \"watch\", \"way\", \"we\", \"wear\", \"weather\", \"week\", \"welcom\", \"went\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"white\", \"who\", \"whole\", \"why\", \"will\", \"win\", \"winter\", \"wish\", \"with\", \"without\", \"woman\", \"won\", \"word\", \"work\", \"world\", \"worst\", \"worth\", \"would\", \"wouldn\", \"wow\", \"write\", \"wrong\", \"wrote\", \"wtf\", \"ya\", \"yea\", \"yeah\", \"year\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yep\", \"yes\", \"yesss\", \"yesterday\", \"yet\", \"yo\", \"you\", \"young\", \"your\", \"youth\", \"yup\", \"zero\", \"zimbabw\", \"zombi\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [19, 1, 20, 14, 10, 18, 11, 8, 12, 13, 17, 5, 6, 15, 4, 16, 9, 7, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el144871397104990165208891797142\", ldavis_el144871397104990165208891797142_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el144871397104990165208891797142\", ldavis_el144871397104990165208891797142_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el144871397104990165208891797142\", ldavis_el144871397104990165208891797142_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "18     0.380374 -0.286813       1        1  24.907717\n",
       "0      0.233929  0.374696       2        1  13.946435\n",
       "19     0.059713  0.023625       3        1   6.386699\n",
       "13     0.128316  0.062891       4        1   6.133837\n",
       "9      0.024128  0.005085       5        1   4.692767\n",
       "17    -0.001301 -0.000620       6        1   4.687242\n",
       "10    -0.035306 -0.009155       7        1   3.634151\n",
       "7     -0.047987 -0.010453       8        1   3.269422\n",
       "11    -0.006389 -0.004409       9        1   3.263285\n",
       "12    -0.068366 -0.013055      10        1   3.126438\n",
       "16    -0.033890 -0.009454      11        1   2.930114\n",
       "4     -0.036453 -0.009629      12        1   2.927520\n",
       "5     -0.057834 -0.012274      13        1   2.877520\n",
       "14    -0.064143 -0.014237      14        1   2.773016\n",
       "3     -0.076237 -0.016294      15        1   2.672491\n",
       "15    -0.051744 -0.011374      16        1   2.666663\n",
       "8     -0.079910 -0.016651      17        1   2.514332\n",
       "6     -0.087963 -0.016878      18        1   2.327947\n",
       "2     -0.078891 -0.016440      19        1   2.260944\n",
       "1     -0.100045 -0.018558      20        1   2.001454, topic_info=     Category           Freq            Term          Total  loglift  logprob\n",
       "term                                                                         \n",
       "36    Default  127281.000000             the  127281.000000  30.0000  30.0000\n",
       "18    Default  117624.000000             you  117624.000000  29.0000  29.0000\n",
       "38    Default   82905.000000              to   82905.000000  28.0000  28.0000\n",
       "22    Default   52513.000000          market   52513.000000  27.0000  27.0000\n",
       "148   Default   55661.000000             and   55661.000000  26.0000  26.0000\n",
       "8     Default   30979.000000             for   30979.000000  25.0000  25.0000\n",
       "444   Default   26065.000000             nfl   26065.000000  24.0000  24.0000\n",
       "116   Default   40416.000000         concert   40416.000000  23.0000  23.0000\n",
       "223   Default   27498.000000            your   27498.000000  22.0000  22.0000\n",
       "31    Default   51379.000000              in   51379.000000  21.0000  21.0000\n",
       "57    Default   45952.000000           elect   45952.000000  20.0000  20.0000\n",
       "175   Default   20924.000000              at   20924.000000  19.0000  19.0000\n",
       "112   Default   14998.000000       terrorist   14998.000000  18.0000  18.0000\n",
       "517   Default   13914.000000             can   13914.000000  17.0000  17.0000\n",
       "95    Default   36045.000000              of   36045.000000  16.0000  16.0000\n",
       "43    Default   35405.000000              is   35405.000000  15.0000  15.0000\n",
       "46    Default   24353.000000              on   24353.000000  14.0000  14.0000\n",
       "209   Default   23146.000000            this   23146.000000  13.0000  13.0000\n",
       "293   Default   12789.000000              go   12789.000000  12.0000  12.0000\n",
       "53    Default   22400.000000            with   22400.000000  11.0000  11.0000\n",
       "136   Default   15392.000000            have   15392.000000  10.0000  10.0000\n",
       "259   Default   11924.000000           about   11924.000000   9.0000   9.0000\n",
       "29    Default   11202.000000             get   11202.000000   8.0000   8.0000\n",
       "233   Default   13445.000000             yes   13445.000000   7.0000   7.0000\n",
       "812   Default   12029.000000           would   12029.000000   6.0000   6.0000\n",
       "41    Default    9981.000000          climat    9981.000000   5.0000   5.0000\n",
       "32    Default   24789.000000              it   24789.000000   4.0000   4.0000\n",
       "107   Default   24758.000000            that   24758.000000   3.0000   3.0000\n",
       "173   Default   12388.000000             all   12388.000000   2.0000   2.0000\n",
       "111   Default   23906.000000             are   23906.000000   1.0000   1.0000\n",
       "...       ...            ...             ...            ...      ...      ...\n",
       "152   Topic20    3961.369385           night    3962.290039   3.9111  -2.7392\n",
       "216   Topic20    2321.506592            hope    2322.427246   3.9109  -3.2735\n",
       "672   Topic20    2311.012207            give    2311.932861   3.9109  -3.2781\n",
       "1331  Topic20    2233.584473            must    2234.505127   3.9109  -3.3121\n",
       "430   Topic20    2069.024902           parti    2069.945557   3.9109  -3.3887\n",
       "1265  Topic20    1915.735229           alway    1916.656006   3.9108  -3.4656\n",
       "347   Topic20    1763.908447             its    1764.829224   3.9108  -3.5482\n",
       "947   Topic20    1747.412842             fan    1748.333618   3.9108  -3.5576\n",
       "660   Topic20    1232.025879            away    1232.946655   3.9105  -3.9071\n",
       "2315  Topic20    1070.496460             fun    1071.417236   3.9104  -4.0476\n",
       "3135  Topic20     933.218567          differ     934.139282   3.9103  -4.1849\n",
       "671   Topic20     822.904053             far     823.824768   3.9102  -4.3107\n",
       "1249  Topic20     801.689819            line     802.610535   3.9101  -4.3368\n",
       "1252  Topic20     677.246460              mr     678.167175   3.9099  -4.5055\n",
       "1083  Topic20     663.634094         twitter     664.554810   3.9099  -4.5258\n",
       "1260  Topic20     641.137878  cinematographi     642.058594   3.9099  -4.5603\n",
       "1536  Topic20     588.660889            wear     589.581604   3.9097  -4.6457\n",
       "1661  Topic20     579.382141            fall     580.302856   3.9097  -4.6615\n",
       "669   Topic20     527.088440          accept     528.009155   3.9096  -4.7561\n",
       "1211  Topic20     459.756042             met     460.676758   3.9093  -4.8928\n",
       "1204  Topic20     454.742493              st     455.663208   3.9093  -4.9038\n",
       "3758  Topic20     438.517181            blue     439.437897   3.9092  -4.9401\n",
       "1224  Topic20     399.821472          custom     400.742188   3.9090  -5.0325\n",
       "1489  Topic20     395.782898         shouldn     396.703613   3.9090  -5.0426\n",
       "662   Topic20     388.893250          church     389.813965   3.9089  -5.0602\n",
       "130   Topic20     386.603363           wrote     387.524078   3.9089  -5.0661\n",
       "1081  Topic20     385.912720          critic     386.833435   3.9089  -5.0679\n",
       "3482  Topic20     372.737183            cook     373.657898   3.9088  -5.1026\n",
       "670   Topic20     339.456482           along     340.377197   3.9086  -5.1962\n",
       "3940  Topic20     333.026703        melbourn     333.947418   3.9085  -5.2153\n",
       "\n",
       "[631 rows x 6 columns], token_table=       Topic      Freq       Term\n",
       "term                             \n",
       "466        8  0.999213        abl\n",
       "259        7  0.999927      about\n",
       "612        8  0.999394    absolut\n",
       "669       20  0.998089     accept\n",
       "1856       9  0.999380        act\n",
       "1735       8  0.999550     actual\n",
       "2215      17  0.998223         ad\n",
       "278        4  0.999832      after\n",
       "110        3  0.999748      again\n",
       "683        9  0.999309    against\n",
       "1103       9  0.999338        ago\n",
       "1736       8  0.999032       agre\n",
       "173        3  0.999898        all\n",
       "3383       5  0.998913      allow\n",
       "1548      11  0.999091     almost\n",
       "1465      12  0.997672       alon\n",
       "670       20  0.995954      along\n",
       "1247       4  0.999457    alreadi\n",
       "174        2  0.999825       also\n",
       "1265      20  0.999658      alway\n",
       "24        10  0.999654         am\n",
       "114       13  0.999518       amaz\n",
       "1602       7  0.999372    america\n",
       "1142      12  0.999286   american\n",
       "147        3  0.999924         an\n",
       "148        2  0.999978        and\n",
       "1361      16  0.997527    announc\n",
       "903        6  0.999572      anoth\n",
       "684        9  0.998788       anti\n",
       "545        7  0.999722        any\n",
       "...      ...       ...        ...\n",
       "9514       4  0.000337        yep\n",
       "9514       5  0.000674        yep\n",
       "9514       6  0.000674        yep\n",
       "9514       7  0.001011        yep\n",
       "9514       8  0.000674        yep\n",
       "9514       9  0.000337        yep\n",
       "9514      10  0.000674        yep\n",
       "9514      11  0.000674        yep\n",
       "9514      12  0.000337        yep\n",
       "9514      13  0.000337        yep\n",
       "9514      14  0.000337        yep\n",
       "9514      15  0.000674        yep\n",
       "9514      16  0.000674        yep\n",
       "9514      17  0.000337        yep\n",
       "9514      18  0.001011        yep\n",
       "9514      19  0.001684        yep\n",
       "9514      20  0.000337        yep\n",
       "233        4  0.999912        yes\n",
       "11334     17  0.998279      yesss\n",
       "1667       2  0.999712  yesterday\n",
       "813        6  0.999695        yet\n",
       "4858       6  0.999862         yo\n",
       "18         1  0.999989        you\n",
       "2939      19  0.999208      young\n",
       "223        4  0.999975       your\n",
       "3075       7  0.998711      youth\n",
       "33118      6  0.999670        yup\n",
       "3387       5  0.999178       zero\n",
       "4311       3  0.998797    zimbabw\n",
       "12862     18  0.997787      zombi\n",
       "\n",
       "[620 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[19, 1, 20, 14, 10, 18, 11, 8, 12, 13, 17, 5, 6, 15, 4, 16, 9, 7, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So how to infer pyLDAvis’s output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "We have successfully built a good looking topic model.\n",
    "\n",
    "Given our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n",
    "\n",
    "#### Upnext, we will improve upon this model by using Mallet’s version of LDA algorithm and then we will focus on how to arrive at the optimal number of topics given any large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LDA Mallet Model\n",
    "So far you have seen Gensim’s inbuilt version of the LDA algorithm. Mallet’s version, however, often gives a better quality of topics.\n",
    "\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. You only need to download the zipfile, unzip it and provide the path to mallet in the unzipped directory to gensim.models.wrappers.LdaMallet. See how I have done this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '../mallet-2.0.8/bin/mallet' # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.57 s, sys: 76.1 ms, total: 6.65 s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldamallet = gensim.models.wrappers.LdaMallet( mallet_path, corpus=corpus, num_topics=20, id2word=id2word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12,\n",
      "  [('terrorist', 0.13547487629561064),\n",
      "   ('terror', 0.10094049618887632),\n",
      "   ('attack', 0.025106562903539145),\n",
      "   ('countri', 0.0193355960094621),\n",
      "   ('white', 0.016901505022455347),\n",
      "   ('kill', 0.01669580719256745),\n",
      "   ('support', 0.015713028671991956),\n",
      "   ('group', 0.013998880089592833),\n",
      "   ('act', 0.013004673911801342),\n",
      "   ('organ', 0.011450512530426137)]),\n",
      " (19,\n",
      "  [('concert', 0.21292937653674807),\n",
      "   ('night', 0.06288190471553425),\n",
      "   ('live', 0.043284563137522214),\n",
      "   ('tonight', 0.03242690556759257),\n",
      "   ('amaz', 0.02383328870170655),\n",
      "   ('music', 0.02163010930691141),\n",
      "   ('life', 0.020023370742751418),\n",
      "   ('perform', 0.015787423619056894),\n",
      "   ('school', 0.013267765416169633),\n",
      "   ('hall', 0.012001850183801155)]),\n",
      " (1,\n",
      "  [('cinema', 0.12372304199772985),\n",
      "   ('watch', 0.05238996090301425),\n",
      "   ('movi', 0.01872871736662883),\n",
      "   ('film', 0.01431454155631227),\n",
      "   ('enjoy', 0.010556186152099887),\n",
      "   ('cinemat', 0.010392231050573842),\n",
      "   ('date', 0.009900365745995713),\n",
      "   ('show', 0.0096229032664901),\n",
      "   ('beauti', 0.009484172026737294),\n",
      "   ('regal', 0.0086139487955606)]),\n",
      " (8,\n",
      "  [('market', 0.18051291521433024),\n",
      "   ('job', 0.07749128598139471),\n",
      "   ('open', 0.03863022552572655),\n",
      "   ('hire', 0.03289397751667782),\n",
      "   ('latest', 0.026834560605710857),\n",
      "   ('click', 0.026292098520348098),\n",
      "   ('work', 0.025449551026061265),\n",
      "   ('manag', 0.023337411417095635),\n",
      "   ('read', 0.021825442625978162),\n",
      "   ('appli', 0.021282980540615407)]),\n",
      " (11,\n",
      "  [('climat', 0.12473402303910779),\n",
      "   ('chang', 0.08380414312617702),\n",
      "   ('world', 0.02566830532932228),\n",
      "   ('real', 0.020752317362486852),\n",
      "   ('polit', 0.019957443686257247),\n",
      "   ('believ', 0.016826864284491403),\n",
      "   ('current', 0.012412258174970039),\n",
      "   ('issu', 0.01117714677036711),\n",
      "   ('caus', 0.009734145327365667),\n",
      "   ('action', 0.008743610438525692)]),\n",
      " (10,\n",
      "  [('concert', 0.30309837146275365),\n",
      "   ('ticket', 0.03934614351637083),\n",
      "   ('wait', 0.022548038186390604),\n",
      "   ('friend', 0.02135166149864492),\n",
      "   ('excit', 0.018372927704665868),\n",
      "   ('buy', 0.015552896940693898),\n",
      "   ('gonna', 0.014576262909881095),\n",
      "   ('wanna', 0.014393144029103693),\n",
      "   ('tonight', 0.012415460116707766),\n",
      "   ('attend', 0.01062089508508924)]),\n",
      " (2,\n",
      "  [('make', 0.033534410066809364),\n",
      "   ('money', 0.02268923915158591),\n",
      "   ('hope', 0.022442757994421744),\n",
      "   ('give', 0.02136602451838879),\n",
      "   ('plan', 0.01730557177142116),\n",
      "   ('campaign', 0.015074268664461309),\n",
      "   ('follow', 0.014192125575663229),\n",
      "   ('tweet', 0.011999740546150354),\n",
      "   ('stop', 0.01198676785366803),\n",
      "   ('support', 0.011805150158915484)]),\n",
      " (13,\n",
      "  [('good', 0.05777439614149569),\n",
      "   ('peopl', 0.05372502998336407),\n",
      "   ('thing', 0.03641850328204996),\n",
      "   ('mani', 0.031453516113640174),\n",
      "   ('anoth', 0.02669486607430716),\n",
      "   ('big', 0.0243219890899243),\n",
      "   ('talk', 0.020620816837109732),\n",
      "   ('lot', 0.020182350437821597),\n",
      "   ('actual', 0.017951330229679017),\n",
      "   ('isn', 0.017061501360535445)]),\n",
      " (17,\n",
      "  [('love', 0.06269973734942877),\n",
      "   ('great', 0.049981239244911956),\n",
      "   ('citi', 0.020546261434356766),\n",
      "   ('show', 0.01996403110403809),\n",
      "   ('littl', 0.015564957497185887),\n",
      "   ('nice', 0.01121763769747312),\n",
      "   ('kid', 0.01121763769747312),\n",
      "   ('fall', 0.011165883890333683),\n",
      "   ('weekend', 0.011049437824269948),\n",
      "   ('book', 0.010674222722509025)]),\n",
      " (0,\n",
      "  [('elect', 0.21595361228647547),\n",
      "   ('trump', 0.05067930032669102),\n",
      "   ('won', 0.027521608621749665),\n",
      "   ('tri', 0.020722577845285885),\n",
      "   ('presid', 0.019529130953672563),\n",
      "   ('lost', 0.015804130049546127),\n",
      "   ('russia', 0.013839161935071667),\n",
      "   ('lose', 0.013441346304533893),\n",
      "   ('rig', 0.01288681542560245),\n",
      "   ('russian', 0.012151459260062928)])]\n",
      "\n",
      "Coherence Score:  0.43027187247896687\n",
      "CPU times: user 4.77 s, sys: 401 ms, total: 5.18 s\n",
      "Wall time: 6.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the optimal number of topics for LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n",
    "Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n",
    "\n",
    "If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "The compute_coherence_values() (see below) trains multiple LDA models and provides the models and their corresponding coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list, coherence_values = compute_coherence_values( dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=40, step=6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.2022\n",
      "Num Topics = 8  has Coherence Value of 0.4414\n",
      "Num Topics = 14  has Coherence Value of 0.4552\n",
      "Num Topics = 20  has Coherence Value of 0.4405\n",
      "Num Topics = 26  has Coherence Value of 0.4478\n",
      "Num Topics = 32  has Coherence Value of 0.4253\n",
      "Num Topics = 38  has Coherence Value of 0.4047\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.173*\"market\" + 0.038*\"stock\" + 0.016*\"point\" + 0.013*\"singl\" + '\n",
      "  '0.011*\"trade\" + 0.011*\"fall\" + 0.010*\"record\" + 0.010*\"public\" + 0.010*\"uk\" '\n",
      "  '+ 0.009*\"drop\"'),\n",
      " (1,\n",
      "  '0.256*\"elect\" + 0.071*\"vote\" + 0.048*\"trump\" + 0.021*\"win\" + 0.020*\"won\" + '\n",
      "  '0.019*\"presid\" + 0.018*\"democrat\" + 0.015*\"lost\" + 0.013*\"russia\" + '\n",
      "  '0.012*\"midterm\"'),\n",
      " (2,\n",
      "  '0.126*\"climat\" + 0.084*\"chang\" + 0.021*\"real\" + 0.019*\"believ\" + '\n",
      "  '0.017*\"polit\" + 0.016*\"world\" + 0.013*\"report\" + 0.013*\"caus\" + '\n",
      "  '0.011*\"issu\" + 0.009*\"isn\"'),\n",
      " (3,\n",
      "  '0.186*\"elect\" + 0.042*\"physic\" + 0.026*\"state\" + 0.023*\"parti\" + '\n",
      "  '0.021*\"result\" + 0.017*\"general\" + 0.014*\"special\" + 0.013*\"candid\" + '\n",
      "  '0.012*\"local\" + 0.011*\"offic\"'),\n",
      " (4,\n",
      "  '0.043*\"realli\" + 0.032*\"concert\" + 0.027*\"fuck\" + 0.026*\"good\" + '\n",
      "  '0.025*\"feel\" + 0.024*\"lol\" + 0.020*\"didn\" + 0.019*\"gonna\" + 0.019*\"someon\" '\n",
      "  '+ 0.018*\"bad\"'),\n",
      " (5,\n",
      "  '0.168*\"market\" + 0.024*\"busi\" + 0.017*\"plan\" + 0.014*\"check\" + '\n",
      "  '0.014*\"event\" + 0.012*\"compani\" + 0.012*\"learn\" + 0.011*\"brand\" + '\n",
      "  '0.011*\"marketplac\" + 0.009*\"drink\"'),\n",
      " (6,\n",
      "  '0.141*\"terrorist\" + 0.039*\"terror\" + 0.027*\"support\" + 0.025*\"countri\" + '\n",
      "  '0.018*\"white\" + 0.017*\"kill\" + 0.017*\"black\" + 0.017*\"american\" + '\n",
      "  '0.015*\"group\" + 0.014*\"america\"'),\n",
      " (7,\n",
      "  '0.075*\"don\" + 0.067*\"make\" + 0.038*\"free\" + 0.030*\"tri\" + 0.025*\"big\" + '\n",
      "  '0.022*\"money\" + 0.022*\"thing\" + 0.020*\"lot\" + 0.019*\"work\" + 0.019*\"doesn\"'),\n",
      " (8,\n",
      "  '0.284*\"nfl\" + 0.041*\"game\" + 0.033*\"team\" + 0.027*\"player\" + 0.025*\"play\" + '\n",
      "  '0.019*\"season\" + 0.015*\"fan\" + 0.014*\"footbal\" + 0.013*\"draft\" + '\n",
      "  '0.011*\"pick\"'),\n",
      " (9,\n",
      "  '0.062*\"love\" + 0.049*\"great\" + 0.027*\"man\" + 0.024*\"show\" + 0.021*\"guy\" + '\n",
      "  '0.017*\"meet\" + 0.015*\"littl\" + 0.014*\"hey\" + 0.014*\"good\" + '\n",
      "  '0.013*\"hungari\"'),\n",
      " (10,\n",
      "  '0.166*\"market\" + 0.079*\"job\" + 0.039*\"open\" + 0.033*\"hire\" + 0.027*\"work\" + '\n",
      "  '0.027*\"latest\" + 0.027*\"click\" + 0.024*\"manag\" + 0.022*\"read\" + '\n",
      "  '0.022*\"appli\"'),\n",
      " (11,\n",
      "  '0.210*\"concert\" + 0.055*\"night\" + 0.044*\"tonight\" + 0.043*\"live\" + '\n",
      "  '0.024*\"amaz\" + 0.021*\"music\" + 0.016*\"perform\" + 0.014*\"school\" + '\n",
      "  '0.012*\"hall\" + 0.012*\"favorit\"'),\n",
      " (12,\n",
      "  '0.074*\"peopl\" + 0.071*\"terror\" + 0.031*\"call\" + 0.028*\"attack\" + '\n",
      "  '0.025*\"mani\" + 0.017*\"happen\" + 0.016*\"stop\" + 0.014*\"act\" + 0.013*\"famili\" '\n",
      "  '+ 0.012*\"thought\"'),\n",
      " (13,\n",
      "  '0.047*\"back\" + 0.040*\"week\" + 0.031*\"anoth\" + 0.018*\"talk\" + 0.016*\"good\" + '\n",
      "  '0.015*\"close\" + 0.015*\"move\" + 0.014*\"stop\" + 0.013*\"turn\" + 0.013*\"run\"'),\n",
      " (14,\n",
      "  '0.076*\"elect\" + 0.029*\"hope\" + 0.029*\"pleas\" + 0.027*\"win\" + 0.026*\"give\" + '\n",
      "  '0.014*\"follow\" + 0.013*\"current\" + 0.012*\"lose\" + 0.012*\"tweet\" + '\n",
      "  '0.011*\"agre\"'),\n",
      " (15,\n",
      "  '0.112*\"day\" + 0.085*\"year\" + 0.075*\"time\" + 0.047*\"today\" + 0.039*\"everi\" + '\n",
      "  '0.038*\"start\" + 0.031*\"ve\" + 0.027*\"sinc\" + 0.022*\"wait\" + 0.020*\"happi\"'),\n",
      " (16,\n",
      "  '0.136*\"market\" + 0.028*\"today\" + 0.027*\"farmer\" + 0.025*\"christma\" + '\n",
      "  '0.020*\"sunday\" + 0.019*\"place\" + 0.016*\"morn\" + 0.016*\"food\" + '\n",
      "  '0.015*\"saturday\" + 0.014*\"street\"'),\n",
      " (17,\n",
      "  '0.142*\"market\" + 0.023*\"hous\" + 0.021*\"home\" + 0.016*\"sell\" + '\n",
      "  '0.016*\"interest\" + 0.015*\"hit\" + 0.014*\"top\" + 0.013*\"buy\" + 0.012*\"set\" + '\n",
      "  '0.011*\"price\"'),\n",
      " (18,\n",
      "  '0.123*\"cinema\" + 0.056*\"watch\" + 0.020*\"citi\" + 0.019*\"movi\" + 0.014*\"film\" '\n",
      "  '+ 0.014*\"full\" + 0.013*\"show\" + 0.010*\"date\" + 0.010*\"walk\" + 0.010*\"time\"'),\n",
      " (19,\n",
      "  '0.273*\"concert\" + 0.039*\"ticket\" + 0.023*\"friend\" + 0.018*\"excit\" + '\n",
      "  '0.016*\"month\" + 0.014*\"tomorrow\" + 0.013*\"miss\" + 0.012*\"girl\" + '\n",
      "  '0.011*\"anyon\" + 0.011*\"wait\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the dominant topic in each sentence\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "The format_topics_sentences() function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 2, placement implies 219941",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-4652deb95dfa>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Add original text to the end of the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msent_topics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_topics_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_topics_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                        raise_cast_failure=True)\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   4675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4677\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4679\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2302\u001b[0m         super(ObjectBlock, self).__init__(values, ndim=ndim,\n\u001b[0;32m-> 2303\u001b[0;31m                                           placement=placement)\n\u001b[0m\u001b[1;32m   2304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    123\u001b[0m             raise ValueError(\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m'Wrong number of items passed {val}, placement implies '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 219941"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most representative document for each topic\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_topic_sents_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_topic_sents_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabular output above actually has 20 rows, one each for a topic. It has the topic number, the keywords, and the most representative document. The Perc_Contribution column is nothing but the percentage contribution of the topic in the given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution across documents\n",
    "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_topic_sents_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7a4e550c3412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Number of Documents for Each Topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtopic_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_topic_sents_keywords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dominant_Topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Percentage of Documents for Each Topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopic_contribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_counts\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtopic_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_topic_sents_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We started with understanding what topic modeling can do. We built a basic topic model using Gensim’s LDA and visualize the topics using pyLDAvis. Then we built mallet’s LDA implementation. You saw how to find the optimal number of topics using coherence scores and how you can come to a logical understanding of how to choose the optimal model.\n",
    "\n",
    "Finally we saw how to aggregate and present the results to generate insights that may be in a more actionable.\n",
    "\n",
    "Hope you enjoyed reading this. I would appreciate if you leave your thoughts in the comments section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUST (!) LOOK AT THIS FOR BETTER VISUALIZATION: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "\n",
    "### T-SNE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
