{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "from polyglot.detect import Detector\n",
    "import icu\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on running the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Source for reading in the gzipped text file:\n",
    "https://stackoverflow.com/questions/23091770/how-do-you-iterate-through-a-gzipped-carriage-return-file-using-python-2-7\n",
    "\n",
    "https://www.makeuseof.com/tag/json-python-parsing-simple-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual:\n",
    "\n",
    "#### Time needed for 2.0 GB file: 20 min -> around 100 CPU hours for all!\n",
    "\n",
    "#### --\n",
    "\n",
    "### Previous:\n",
    "\n",
    "#### Only reading the file and count its rows: \n",
    "\n",
    "* CPU times: user 1min 8s, sys: 656 ms, total: 1min 9s.\n",
    "* Wall time: <font color='red'>1min 9s</font>, approx. <font color='green'>30 MB/s</font>.\n",
    "\n",
    "        %%time\n",
    "        i=0\n",
    "        with io.TextIOWrapper( io.BufferedReader(gzip.open(filename)) ) as file:\n",
    "            for line in file:\n",
    "                i += 1\n",
    "        file.close()\n",
    "        print(i)\n",
    "\n",
    "\n",
    "#### Reading in compressed zip and parse lines to json\n",
    "\n",
    "* CPU times: user 4min 36s, sys: 886 ms, total: 4min 37s.\n",
    "* Wall time: <font color='red'>4min 37s</font>, approx. <font color='green'>7.5 MB/s</font>.\n",
    "\n",
    "        %%time\n",
    "        with io.TextIOWrapper( io.BufferedReader(gzip.open(filename)) ) as file:\n",
    "            for line in file:\n",
    "                json_parsed = json.loads(line)\n",
    "        file.close()\n",
    "\n",
    "Number of rows: 3.764.667, file size (compressed): 2.0 GB (1.989.239.557 bytes).\n",
    "\n",
    "#### Doing the filtering process and saving back to a compressed text file\n",
    "* Input: World_171016_111937.txt.gz. output: World_171016_111937_filtered.txt.gz.\n",
    "\n",
    "* CPU times: user 6min 36s, sys: 1.12 s, total: 6min 37s.\n",
    "* Wall time: <font color='red'>6min 38s</font>,  approx. <font color='green'>5 MB/s</font>.\n",
    "\n",
    "* Input: number of rows: 3.764.667, file size (compressed): 2.0 GB (1.989.239.557 bytes).\n",
    "* Output: number of rows: 1.588.596, only text kept, file size (compressed): 66.5 MB (66.549.841 bytes).\n",
    "\n",
    "#### Doing the filtering process for all the compressed text files\n",
    "* Input, output: each separate file is read, filtered and then saved to compressed txt.\n",
    "* According to the estimations 1 thread works with a speed of <font color='green'>5 MB/s</font>, so the <b><font color='red'>CPU time required is about 35-40 hours</font></b>.\n",
    "* <b><font color='brown'>4 jupyter kernels are launched with a given range of filenames to work on to speed up the process.</font></b>\n",
    "* <b><font color='purple'>The memory consumption of each kernel is about 100 MB</font></b>!\n",
    "* After the process ended, the CPU time of 1 thread (the first 1-2 hours not shown, because threads had to be rearranged):\n",
    "\n",
    "    * CPU times: user 8h 25min 50s, sys: 1min 47s, total: 8h 27min 37s.\n",
    "    * Wall time: <font color='red'>8h 35min 59s</font>.\n",
    "* Output after merging (wall time: <font color='red'>1h 44min 16s</font>): number of rows: <font color='red'>492.866.843</font>, file size (compressed): 20.7 GB (20.690.310.158 bytes).\n",
    "\n",
    "#### Number of twitter messages to build the network with: <font color='blue'>492.866.843</font>.\n",
    "\n",
    "#### --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source for language detection:\n",
    "\n",
    "https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language\n",
    "\n",
    "##### Package installation:\n",
    "https://polyglot.readthedocs.io/en/latest/Installation.html\n",
    "\n",
    "#### Removing urls:\n",
    "https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    484 World_181119_100900.txt.gz\r\n",
      "    268 World_181119_100944.txt.gz\r\n",
      "    332 World_181119_101008.txt.gz\r\n",
      "    184 World_181119_101040.txt.gz\r\n",
      "    340 World_181119_101103.txt.gz\r\n",
      "    580 World_181119_101127.txt.gz\r\n",
      "    584 World_181119_101224.txt.gz\r\n",
      "   1752 World_181119_101308.txt.gz\r\n",
      "   1604 World_181119_101536.txt.gz\r\n",
      "    348 World_181119_101755.txt.gz\r\n"
     ]
    }
   ],
   "source": [
    "! ls -s /media/abiricz/Elements/twitter/data/ | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/abiricz/Elements/twitter/data/World_171016_103536.txt.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small file for testing\n",
    "filename = '/media/abiricz/Elements/twitter/data/World_171016_103536.txt.gz'\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 ms, sys: 1.16 ms, total: 39.1 ms\n",
      "Wall time: 39.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with io.TextIOWrapper( io.BufferedReader(gzip.open(filename)) ) as file:\n",
    "    for line in file:\n",
    "        json_parsed = json.loads(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'extended_entities', 'favorited', 'retweeted', 'possibly_sensitive', 'filter_level', 'lang', 'timestamp_ms'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parsed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republic of Korea\n"
     ]
    }
   ],
   "source": [
    "print(json_parsed['place']['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[126.954584, 37.560865],\n",
       "  [126.954584, 37.632942],\n",
       "  [127.020042, 37.632942],\n",
       "  [127.020042, 37.560865]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parsed['place']['bounding_box']['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " '다 좋은데, 사무실 들어가다 갑자기 만나 시위대 행진으로 버스가 안다님. 계속 대기중. #경복궁역 https://t.co/dfAjFATB47')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = json_parsed['text']\n",
    "type(tweet), tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering out url links from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다 좋은데, 사무실 들어가다 갑자기 만나 시위대 행진으로 버스가 안다님. 계속 대기중. #경복궁역 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = re.sub(r'http\\S+', '', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking language detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Korean      code: ko       confidence:  99.0 read bytes:  3676\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n"
     ]
    }
   ],
   "source": [
    "for language in Detector(tweet).languages:\n",
    "        print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(Detector(tweet).languages[0].locale ) # best guess, if 'un' then unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parsed['lang'] # this info comes from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Detector(tweet).languages[0].confidence # confidence of best guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing retweets this way\n",
    "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n",
    "\"Users can amplify the broadcast of Tweets authored by other users by retweeting . Retweets can be distinguished from typical Tweets by the existence of a <font color='red'>retweeted_status</font> attribute. This attribute contains a representation of the original Tweet that was retweeted. Note that retweets of retweets do not show representations of the intermediary retweet, but only the original Tweet. (Users can also unretweet a retweet they created by deleting their retweet.)\"\n",
    "\n",
    "#### Turned out, keeping them, but filtering duplicates yields better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in list(json_parsed.keys()) # indicator if tweet is a retweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the filtering process to be able to work with large files -> functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detector(text):\n",
    "    '''Returns language type as string or 'un' if unknown.'''\n",
    "    try:\n",
    "        polyglot = Detector(text, quiet=True).languages[0]\n",
    "        if polyglot.confidence > 90: # threshold for detecting languages\n",
    "            lang = str(polyglot.locale)\n",
    "        else:\n",
    "            lang = 'un'\n",
    "    except Exception as e:\n",
    "        print(\"Error: \" + str(e))\n",
    "        lang = 'un' # if error occurs during conversion language is unknown\n",
    "    # print('Guess: ', lang)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_language_filtering( text ):\n",
    "    lang = language_detector(text)\n",
    "    if( lang == 'en' ):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural language processing\n",
    "\n",
    "https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokenizer for tweets\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Stemmer\n",
    "snow = SnowballStemmer( 'english', ignore_stopwords=True )\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_regex = ['nfl', 'hungary', 'climate', 'election', 'physics', 'concert', \n",
    "            'market', 'data\\s?mining', 'cinema', 'terror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_tokenized input: one tweet, output: list of its normal words\n",
    "def text_tokenizer( text ):\n",
    "    tokenized_text = []\n",
    "    keyword_match_marker = []    # will contain the exact indices of the matching keywords in the tweet\n",
    "    \n",
    "    tokenized = tweet_tokenizer.tokenize(text)\n",
    "    for item in tokenized:\n",
    "        match = re.match('#?[A-Za-z]+', item)\n",
    "        if(match):\n",
    "            result = match.group(0) # gives back the matching string\n",
    "            # searching for keywords\n",
    "            i = 0    # will mark the index of the matching keyword in the keywords_regex list\n",
    "            for word in keywords_regex:\n",
    "                word_match = re.match(word, result)\n",
    "                if (word_match):\n",
    "                    keyword_match_marker.append(i)\n",
    "                i += 1\n",
    "            \n",
    "            url_filter = re.match('https?', result)\n",
    "            if( url_filter == None ):\n",
    "                real_word = re.findall('\\w+', result)[0]\n",
    "                tokenized_text.append(real_word)\n",
    "    return keyword_match_marker, [ x for x in tokenized_text if ( len(x) > 1 ) ] # REMOVE words that are only 1 letter long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemmer( tokenized_text ):\n",
    "    return [ snow.stem(word) for word in tokenized_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_no_stopwords( tokenized_text ):\n",
    "    return [ w for w in tokenized_text if not w in stop_words ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: @, #, numbers, and non-alphabetic characters are filtered, then the text is tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tweet_processing( filename_in, out_files ):\n",
    "    print('Input:', filename_in)\n",
    "\n",
    "    with io.TextIOWrapper( io.BufferedReader(gzip.open(filename_in)) ) as file_in:\n",
    "        for line in file_in: # read one line at a time from a compressed file\n",
    "            json_parsed = json.loads(line) # parse only that one line to json\n",
    "            tweet = json_parsed['text'] # get tweet message\n",
    "            \n",
    "            keyword_match_marker, tweet_tokenized = text_tokenizer(tweet) # tokenize tweet message\n",
    "            tweet_stemmed = text_stemmer( tweet_tokenized ) # stem tokenized words\n",
    "            # tweet_filtered = text_no_stopwords( tweet_stemmed ) # remove stop words\n",
    "            \n",
    "            lang_bool = English_language_filtering( ' '.join(tweet_tokenized) ) # True: English, False: not English\n",
    "            \n",
    "            if ( lang_bool == True and json_parsed['lang'] == 'en' and \n",
    "                len(keyword_match_marker) != 0 ): # if conditions are met write to file\n",
    "                \n",
    "                for index in keyword_match_marker:\n",
    "                    out_files[index].write( ' '.join(tweet_stemmed).encode() ) # write to file\n",
    "                    out_files[index].write( '\\n'.encode() ) # write a newline char to the end\n",
    "            \n",
    "    file_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale I/O operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all file names in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/media/abiricz/Elements/twitter/data/'\n",
    "target_path = '/media/abiricz/Elements/twitter/keyfilter'+str(thread)+'/'\n",
    "dir_files = [f for f in listdir(dir_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating output files for each keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keywords_regex = ['nfl', 'hungary', 'climate', 'election', 'physics', 'concert', \n",
    "            'market', 'data\\s?mining', 'cinema', 'terror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nfl = gzip.open(target_path+'nfl.txt.gz', mode='wb')\n",
    "file_hungary = gzip.open(target_path+'hungary.txt.gz', mode='wb')\n",
    "file_climate = gzip.open(target_path+'climate.txt.gz', mode='wb')\n",
    "file_election = gzip.open(target_path+'election.txt.gz', mode='wb')\n",
    "file_physics = gzip.open(target_path+'physics.txt.gz', mode='wb')\n",
    "file_concert = gzip.open(target_path+'concert.txt.gz', mode='wb')\n",
    "file_market = gzip.open(target_path+'market.txt.gz', mode='wb')\n",
    "file_data_mining = gzip.open(target_path+'data_mining.txt.gz', mode='wb')\n",
    "file_cinema = gzip.open(target_path+'cinema.txt.gz', mode='wb')\n",
    "file_terror = gzip.open(target_path+'terror.txt.gz', mode='wb')\n",
    "\n",
    "out_files = [file_nfl, file_hungary, file_climate, file_election, file_physics, file_concert,\n",
    "             file_market, file_data_mining, file_cinema, file_terror]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3595,\n",
       " ['World_171016_103536.txt.gz',\n",
       "  'World_171016_104514.txt.gz',\n",
       "  'World_171016_104518.txt.gz'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dir_files), dir_files[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World_171116_003527.txt.gz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_files[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working on: World_171016_103536.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171016_103536.txt.gz\n",
      "CPU times: user 146 ms, sys: 0 ns, total: 146 ms\n",
      "Wall time: 145 ms\n",
      "Currently working on: World_171016_104514.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171016_104514.txt.gz\n",
      "CPU times: user 30.1 ms, sys: 1.55 ms, total: 31.7 ms\n",
      "Wall time: 31.9 ms\n",
      "Currently working on: World_171016_104518.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171016_104518.txt.gz\n",
      "CPU times: user 28.7 ms, sys: 0 ns, total: 28.7 ms\n",
      "Wall time: 28.5 ms\n",
      "Currently working on: World_171016_111937.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171016_111937.txt.gz\n",
      "CPU times: user 32min 45s, sys: 4.29 s, total: 32min 49s\n",
      "Wall time: 33min 31s\n",
      "Currently working on: World_171017_111937.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171017_111937.txt.gz\n",
      "CPU times: user 12min 31s, sys: 1.17 s, total: 12min 33s\n",
      "Wall time: 12min 43s\n",
      "Currently working on: World_171017_200041.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171017_200041.txt.gz\n",
      "CPU times: user 13.1 s, sys: 16 ms, total: 13.2 s\n",
      "Wall time: 13.2 s\n",
      "Currently working on: World_171017_201410.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171017_201410.txt.gz\n",
      "CPU times: user 33min 36s, sys: 4.13 s, total: 33min 41s\n",
      "Wall time: 34min 1s\n",
      "Currently working on: World_171018_201410.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171018_201410.txt.gz\n",
      "CPU times: user 28min 56s, sys: 3.75 s, total: 29min\n",
      "Wall time: 29min 21s\n",
      "Currently working on: World_171019_171639.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171019_171639.txt.gz\n",
      "CPU times: user 17min 14s, sys: 3.02 s, total: 17min 17s\n",
      "Wall time: 17min 28s\n",
      "Currently working on: World_171020_042735.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171020_042735.txt.gz\n",
      "CPU times: user 33min 12s, sys: 3.79 s, total: 33min 15s\n",
      "Wall time: 33min 38s\n",
      "Currently working on: World_171021_042735.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171021_042735.txt.gz\n",
      "CPU times: user 32min 17s, sys: 3.05 s, total: 32min 20s\n",
      "Wall time: 32min 35s\n",
      "Currently working on: World_171022_042735.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171022_042735.txt.gz\n",
      "CPU times: user 32min 26s, sys: 4.66 s, total: 32min 30s\n",
      "Wall time: 32min 49s\n",
      "Currently working on: World_171023_042735.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171023_042735.txt.gz\n",
      "CPU times: user 33min 3s, sys: 3.52 s, total: 33min 7s\n",
      "Wall time: 33min 24s\n",
      "Currently working on: World_171024_042735.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171024_042735.txt.gz\n",
      "CPU times: user 31min 7s, sys: 3.29 s, total: 31min 10s\n",
      "Wall time: 31min 28s\n",
      "Currently working on: World_171025_033944.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171025_033944.txt.gz\n",
      "CPU times: user 309 ms, sys: 7.99 ms, total: 317 ms\n",
      "Wall time: 348 ms\n",
      "Currently working on: World_171025_034006.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171025_034006.txt.gz\n",
      "CPU times: user 6.64 s, sys: 23.9 ms, total: 6.66 s\n",
      "Wall time: 6.76 s\n",
      "Currently working on: World_171025_034442.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171025_034442.txt.gz\n",
      "CPU times: user 33min 24s, sys: 4.2 s, total: 33min 28s\n",
      "Wall time: 34min 16s\n",
      "Currently working on: World_171026_034442.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171026_034442.txt.gz\n",
      "CPU times: user 22min 31s, sys: 2.3 s, total: 22min 33s\n",
      "Wall time: 22min 46s\n",
      "Currently working on: World_171026_202938.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171026_202938.txt.gz\n",
      "CPU times: user 33min 41s, sys: 3.9 s, total: 33min 45s\n",
      "Wall time: 34min 6s\n",
      "Currently working on: World_171027_202938.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171027_202938.txt.gz\n",
      "CPU times: user 32min 31s, sys: 4.06 s, total: 32min 35s\n",
      "Wall time: 32min 54s\n",
      "Currently working on: World_171028_202938.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171028_202938.txt.gz\n",
      "CPU times: user 33min 23s, sys: 3.96 s, total: 33min 27s\n",
      "Wall time: 33min 50s\n",
      "Currently working on: World_171029_202938.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171029_202938.txt.gz\n",
      "CPU times: user 33min 4s, sys: 3.15 s, total: 33min 7s\n",
      "Wall time: 33min 27s\n",
      "Currently working on: World_171030_202938.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171030_202938.txt.gz\n",
      "CPU times: user 23min 18s, sys: 2.21 s, total: 23min 20s\n",
      "Wall time: 23min 33s\n",
      "Currently working on: World_171031_135915.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171031_135915.txt.gz\n",
      "CPU times: user 403 ms, sys: 7 µs, total: 403 ms\n",
      "Wall time: 442 ms\n",
      "Currently working on: World_171031_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171031_135932.txt.gz\n",
      "CPU times: user 33min 14s, sys: 3.86 s, total: 33min 18s\n",
      "Wall time: 33min 44s\n",
      "Currently working on: World_171101_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171101_135932.txt.gz\n",
      "CPU times: user 33min 45s, sys: 3.68 s, total: 33min 49s\n",
      "Wall time: 34min 16s\n",
      "Currently working on: World_171102_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171102_135932.txt.gz\n",
      "CPU times: user 33min 42s, sys: 3.49 s, total: 33min 45s\n",
      "Wall time: 34min 6s\n",
      "Currently working on: World_171103_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171103_135932.txt.gz\n",
      "CPU times: user 32min 46s, sys: 3.42 s, total: 32min 49s\n",
      "Wall time: 33min 18s\n",
      "Currently working on: World_171105_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171105_135932.txt.gz\n",
      "CPU times: user 32min 51s, sys: 3.82 s, total: 32min 55s\n",
      "Wall time: 33min 12s\n",
      "Currently working on: World_171106_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171106_135932.txt.gz\n",
      "CPU times: user 33min 20s, sys: 4.45 s, total: 33min 25s\n",
      "Wall time: 33min 45s\n",
      "Currently working on: World_171107_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171107_135932.txt.gz\n",
      "CPU times: user 15min 33s, sys: 1.94 s, total: 15min 34s\n",
      "Wall time: 15min 44s\n",
      "Currently working on: World_171107_235214.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171107_235214.txt.gz\n",
      "CPU times: user 35min 5s, sys: 3.51 s, total: 35min 9s\n",
      "Wall time: 35min 26s\n",
      "Currently working on: World_171108_235214.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171108_235214.txt.gz\n",
      "CPU times: user 8.06 s, sys: 24 ms, total: 8.09 s\n",
      "Wall time: 8.29 s\n",
      "Currently working on: World_171108_235720.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171108_235720.txt.gz\n",
      "CPU times: user 17.2 s, sys: 35.9 ms, total: 17.3 s\n",
      "Wall time: 17.6 s\n",
      "Currently working on: World_171109_000813.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171109_000813.txt.gz\n",
      "CPU times: user 42.9 s, sys: 63.9 ms, total: 43 s\n",
      "Wall time: 43.2 s\n",
      "Currently working on: World_171109_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171109_003526.txt.gz\n",
      "CPU times: user 34min 33s, sys: 4.24 s, total: 34min 37s\n",
      "Wall time: 34min 57s\n",
      "Currently working on: World_171110_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171110_003526.txt.gz\n",
      "CPU times: user 34min 18s, sys: 4 s, total: 34min 22s\n",
      "Wall time: 34min 50s\n",
      "Currently working on: World_171111_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171111_003526.txt.gz\n",
      "CPU times: user 32min 32s, sys: 4.67 s, total: 32min 37s\n",
      "Wall time: 32min 54s\n",
      "Currently working on: World_171112_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171112_003526.txt.gz\n",
      "CPU times: user 32min 24s, sys: 4.1 s, total: 32min 28s\n",
      "Wall time: 33min\n",
      "Currently working on: World_171113_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171113_003526.txt.gz\n",
      "CPU times: user 33min 1s, sys: 3.9 s, total: 33min 5s\n",
      "Wall time: 33min 27s\n",
      "Currently working on: World_171114_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171114_003526.txt.gz\n",
      "CPU times: user 33min 37s, sys: 3.8 s, total: 33min 41s\n",
      "Wall time: 34min 10s\n",
      "Currently working on: World_171115_003526.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171115_003526.txt.gz\n",
      "CPU times: user 33min 35s, sys: 3.78 s, total: 33min 38s\n",
      "Wall time: 33min 54s\n",
      "Currently working on: World_171117_003527.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171117_003527.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 18s, sys: 2.81 s, total: 29min 21s\n",
      "Wall time: 29min 36s\n",
      "Currently working on: World_171117_205234.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171117_205234.txt.gz\n",
      "CPU times: user 8.62 s, sys: 12 ms, total: 8.63 s\n",
      "Wall time: 8.8 s\n",
      "Currently working on: World_171117_205801.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171117_205801.txt.gz\n",
      "CPU times: user 43.3 s, sys: 95.8 ms, total: 43.4 s\n",
      "Wall time: 44 s\n",
      "Currently working on: World_171117_212515.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171117_212515.txt.gz\n",
      "CPU times: user 33min 52s, sys: 4.13 s, total: 33min 56s\n",
      "Wall time: 34min 17s\n",
      "Currently working on: World_171118_212515.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171118_212515.txt.gz\n",
      "CPU times: user 33min 49s, sys: 4.01 s, total: 33min 53s\n",
      "Wall time: 34min 12s\n",
      "Currently working on: World_171119_212515.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171119_212515.txt.gz\n",
      "CPU times: user 33min 42s, sys: 3.22 s, total: 33min 45s\n",
      "Wall time: 33min 53s\n",
      "Currently working on: World_171120_212515.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171120_212515.txt.gz\n",
      "CPU times: user 20min 36s, sys: 1.83 s, total: 20min 38s\n",
      "Wall time: 20min 38s\n",
      "Currently working on: World_171121_153909.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171121_153909.txt.gz\n",
      "CPU times: user 29min 13s, sys: 2.51 s, total: 29min 16s\n",
      "Wall time: 29min 19s\n",
      "Currently working on: World_171122_153909.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171122_153909.txt.gz\n",
      "CPU times: user 54.9 s, sys: 64 ms, total: 55 s\n",
      "Wall time: 55 s\n",
      "Currently working on: World_171122_162254.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171122_162254.txt.gz\n",
      "CPU times: user 29 s, sys: 52 ms, total: 29 s\n",
      "Wall time: 29 s\n",
      "Currently working on: World_171122_165659.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171122_165659.txt.gz\n",
      "CPU times: user 26min 28s, sys: 2.18 s, total: 26min 30s\n",
      "Wall time: 26min 31s\n",
      "Currently working on: World_171123_165659.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171123_165659.txt.gz\n",
      "CPU times: user 21min 54s, sys: 1.85 s, total: 21min 56s\n",
      "Wall time: 21min 57s\n",
      "Currently working on: World_171124_165659.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171124_165659.txt.gz\n",
      "CPU times: user 19min 45s, sys: 2.1 s, total: 19min 47s\n",
      "Wall time: 19min 47s\n",
      "Currently working on: World_171125_165659.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171125_165659.txt.gz\n",
      "CPU times: user 18min 45s, sys: 1.62 s, total: 18min 47s\n",
      "Wall time: 18min 47s\n",
      "Currently working on: World_171025_030255.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171025_030255.txt.gz\n",
      "CPU times: user 32.4 s, sys: 32 ms, total: 32.5 s\n",
      "Wall time: 32.5 s\n",
      "Currently working on: World_171104_135932.txt.gz\n",
      "Input: /media/abiricz/Elements/twitter/data/World_171104_135932.txt.gz\n",
      "CPU times: user 18min 46s, sys: 1.59 s, total: 18min 48s\n",
      "Wall time: 18min 48s\n",
      "CPU times: user 21h 13min 58s, sys: 2min 23s, total: 21h 16min 22s\n",
      "Wall time: 21h 28min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in dir_files[a:b]:\n",
    "    print('Currently working on:', i)\n",
    "    %time do_tweet_processing( dir_path+i, out_files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in out_files:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
