{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_nodupl.txt.gz   each_file_no_dupl.zip  keyfilter2  market.txt.gz\r\n",
      "all.txt.gz\t    each_file.zip\t   keyfilter3  nfl.txt.gz\r\n",
      "cinema.txt.gz\t    election.txt.gz\t   keyfilter4  physics.txt.gz\r\n",
      "climate.txt.gz\t    hungary.txt.gz\t   keyfilter5  terror.txt.gz\r\n",
      "concert.txt.gz\t    keyfilter0\t\t   keyfilter6\r\n",
      "data_mining.txt.gz  keyfilter1\t\t   keyfilter7\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 s, sys: 170 ms, total: 3.5 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv( '../Data_process/all_nodupl.txt.gz', names=['Tweets'], compression='gzip' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              Tweets\n",
       " 0  aaaaaaaaaaaaaa reveng of the sith palaba sa ci...\n",
       " 1  aaaaaaaaaahhhhhh can hide my excit be see you ...\n",
       " 2  aaaaaaaa could paus stan ikon don wanna be sad...\n",
       " 3                                aaaaaaa miss physic\n",
       " 4  aaaaaa grabe thank you lord afford po ticket p...,\n",
       "                                                     Tweets\n",
       " 2289719  zzzquil is benadryl diphenhydramin the same ex...\n",
       " 2289720  zzz select data use to promulg climat antarct ...\n",
       " 2289721    zzzz and peopl rip on mlb and nfl replay review\n",
       " 2289722  zzzz como on mike for bcash to do wel you don ...\n",
       " 2289723  zzzz the climat is primarili dictat by solar c...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(), data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 s, sys: 500 ms, total: 4.23 s\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_words = []\n",
    "for i in range( len(data_list) ):\n",
    "    list_of_words.append( data_list[i][0].split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2289724,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( list_of_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering other kind of duplicates (duplicates are already filtered with uniq after sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 982 ms, sys: 0 ns, total: 982 ms\n",
      "Wall time: 983 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_words_filtered = []\n",
    "for i in range( 1, len(list_of_words) ):\n",
    "    if len(list_of_words[i]) == len(list_of_words[i-1])+1: # extra word at the end\n",
    "        if ' '.join(list_of_words[i][:-1]) == ' '.join(list_of_words[i-1]): # difference only in last word\n",
    "            next # skip this line\n",
    "    else:\n",
    "        list_of_words_filtered.append( list_of_words[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2124173,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( list_of_words_filtered )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open( '../Data/All_filtered.txt', mode='w' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for i in range( len(list_of_words_filtered) ):\n",
    "#    file.write( ' '.join(list_of_words_filtered[i]) )\n",
    "#    file.write( '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create suitable sized dataset from parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [ 'cinema', 'climate', 'concert', 'data_mining', 'election', \n",
    "             'hungary', 'market', 'nfl', 'physics', 'terror' ]\n",
    "dirnames = [ 'keyfilter0', 'keyfilter1', 'keyfilter2', 'keyfilter3', 'keyfilter4', \n",
    "            'keyfilter5', 'keyfilter6', 'keyfilter7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( '../Data_process/'+dirnames[0]+'/'+filenames[0]+'.txt', names=['Tweets'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good morn yeah had great weekend got day off a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>victoria and muhammad sf world cinema in pathu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at golden screen cinema gsc in melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youth team at work earli this morn on set at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman just came and sat right next to me in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  good morn yeah had great weekend got day off a...\n",
       "1  victoria and muhammad sf world cinema in pathu...\n",
       "2              at golden screen cinema gsc in melaka\n",
       "3  youth team at work earli this morn on set at l...\n",
       "4  woman just came and sat right next to me in th..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter exact duplicate lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19681,),\n",
       " (17458,),\n",
       " 'good morn yeah had great weekend got day off and enjoy the beuti weather and went to the cinemar')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data['Tweets']), np.shape( data['Tweets'].unique() ), data['Tweets'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morn', 'yeah', 'had', 'great', 'weekend', 'got', 'day', 'off', 'and', 'enjoy', 'the', 'beuti', 'weather', 'and', 'went', 'to', 'the', 'cinemar']\n",
      "CPU times: user 227 ms, sys: 0 ns, total: 227 ms\n",
      "Wall time: 225 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_words = []\n",
    "data_unique = data['Tweets'].unique()\n",
    "for i in range( len(data_unique) ):\n",
    "    list_of_words.append( data_unique[i].split() )\n",
    "print( list_of_words[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter same tweets differing only in the last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 560 ms, sys: 116 ms, total: 676 ms\n",
      "Wall time: 673 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_words_filtered = []\n",
    "list_of_words_filtered.append( list_of_words[0] ) # add first row by hand\n",
    "for i in range( 1, len(list_of_words) ):\n",
    "    if len(list_of_words[i]) == len(list_of_words[i-1])+1: # extra word at the end\n",
    "        if ' '.join(list_of_words[i][:-1]) == ' '.join(list_of_words[i-1]): # difference only in last word\n",
    "            next # skip this line\n",
    "    else:\n",
    "        list_of_words_filtered.append( list_of_words[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16445,),\n",
       " ['good',\n",
       "  'morn',\n",
       "  'yeah',\n",
       "  'had',\n",
       "  'great',\n",
       "  'weekend',\n",
       "  'got',\n",
       "  'day',\n",
       "  'off',\n",
       "  'and',\n",
       "  'enjoy',\n",
       "  'the',\n",
       "  'beuti',\n",
       "  'weather',\n",
       "  'and',\n",
       "  'went',\n",
       "  'to',\n",
       "  'the',\n",
       "  'cinemar'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape( list_of_words_filtered ), list_of_words_filtered[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_no_stopwords( tokenized_text ):\n",
    "    return [ w for w in tokenized_text if not w in stop_words ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'morn',\n",
       " 'yeah',\n",
       " 'great',\n",
       " 'weekend',\n",
       " 'got',\n",
       " 'day',\n",
       " 'enjoy',\n",
       " 'beuti',\n",
       " 'weather',\n",
       " 'went',\n",
       " 'cinemar']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_stopwords( list_of_words_filtered[0] ) # remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_save( file_input, file_output, topic, every=1 ):\n",
    "    data = pd.read_csv( file_input, names=['Tweets'] )\n",
    "    \n",
    "    # Filter exact duplicate lines\n",
    "    list_of_words = []\n",
    "    data_unique = data['Tweets'].unique()\n",
    "    for i in range( len(data_unique) ):\n",
    "        list_of_words.append( data_unique[i].split() )\n",
    "    \n",
    "    # Filter same tweets differing only in the last word\n",
    "    list_of_words_filtered = []\n",
    "    list_of_words_filtered.append( list_of_words[0] ) # add first row by hand\n",
    "    for i in range( 1, len(list_of_words) ):\n",
    "        if len(list_of_words[i]) == len(list_of_words[i-1])+1: # extra word at the end\n",
    "            if ' '.join(list_of_words[i][:-1]) == ' '.join(list_of_words[i-1]): # difference only in last word\n",
    "                next # skip this line\n",
    "        else:\n",
    "            list_of_words_filtered.append( list_of_words[i] )\n",
    "    \n",
    "    # Remove stopwords\n",
    "    for i in range( len(list_of_words_filtered) ):\n",
    "        list_of_words_filtered[i] = text_no_stopwords( list_of_words_filtered[i] )\n",
    "    \n",
    "    # Write to file\n",
    "    for i in range( len(list_of_words_filtered) ):\n",
    "        if i % every == 0:\n",
    "            file_output.write( ' '.join(list_of_words_filtered[i]).encode() ) # tweet\n",
    "            file_output.write( ('\\t'+topic).encode() ) # target\n",
    "            file_output.write( '\\n'.encode() ) # newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write each topic to separate files gzipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 s, sys: 62.4 ms, total: 49.2 s\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# open all output files\n",
    "file_output_list = []\n",
    "for i in filenames: \n",
    "    file_output_list.append( gzip.open('../Data_process/'+i+'_nostop.txt.gz', mode='wb') )\n",
    "\n",
    "# do the process\n",
    "for k in range( len(dirnames) ):\n",
    "    for l in range( len(filenames) ):\n",
    "        load_process_save( file_input='../Data_process/'+dirnames[k]+'/'+filenames[l]+'.txt', \n",
    "                          file_output=file_output_list[l], \n",
    "                          topic=filenames[l],\n",
    "                          every=1 )\n",
    "# close all output files\n",
    "for i in file_output_list:\n",
    "    i.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write each topic to one file (only EVERY 10 tweet is written to file: $2 \\cdot 10^5$ documents kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 76.6 ms, total: 14.2 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# open the output file\n",
    "file_output = gzip.open('../Data/Few_nostop.txt.gz', mode='wb')\n",
    "\n",
    "# do the process\n",
    "for k in range( len(dirnames) ):\n",
    "    for l in range( len(filenames) ):\n",
    "        load_process_save( file_input='../Data_process/'+dirnames[k]+'/'+filenames[l]+'.txt', \n",
    "                          file_output=file_output, \n",
    "                          topic=filenames[l],\n",
    "                          every=100 )\n",
    "# close the output file\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After uniting the files it is needed to filter duplicates again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gunzip ../Data/Few_nostop.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22113 ../Data/Few_nostop.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../Data/Few_nostop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Few_filtered.txt.gz  ../Data/Few_nostop.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Data/Few*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the last 20 tweets to console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zac brown band extrem generous perform benefit concert gatlinb\tconcert\r\n",
      "zack snyder cinematograph though work larri fong except cinematograph\tcinema\r\n",
      "zanfona nois concert atlanta ashevill negativeland insid proyect nigthmar noi\tconcert\r\n",
      "zanu primari elect figur drop signific term number voter particip co\telection\r\n",
      "zavala world zavala world elect time excel\telection\r\n",
      "zeke could process clear name expos nfl\tnfl\r\n",
      "zero desir watch nfl hand ne anoth sb\tnfl\r\n",
      "zero emot invest marvel cinemat univers super conveni would recommend\tcinema\r\n",
      "zero evid outcom elect\telection\r\n",
      "zero nra member commit terrorist event mass shoot terrorist attack\tterror\r\n",
      "zilker media digit market team confer week follow\tmarket\r\n",
      "zimbabw armi bloodless coup armi pave way democrat fraud free peac elect\telection\r\n",
      "zimbo master trade onlin elect analysi constitut law cholera epidermiolog\telection\r\n",
      "zim elect joint iri ndi elect observ mission statement constitut court decis\telection\r\n",
      "zoe duck watch parti ducksthehallstmp edward marketplac stadium\tmarket\r\n",
      "zombi hurrican good cinema\tcinema\r\n",
      "zombi mmmm starbuck market\tmarket\r\n",
      "zoo twilight concert watch finn brother belt crowd hous classic great night\tconcert\r\n",
      "zot foam cook teach cook foam physic behind\tphysics\r\n",
      "zuck understand facebook one facebook sat build elect interfer function\telection\r\n"
     ]
    }
   ],
   "source": [
    "!sort ../Data/Few_nostop.txt | uniq | cat | tail -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sort ../Data/Few_nostop.txt | uniq | cat >> ../Data/Few_filtered_nostop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22113 ../Data/Few_nostop.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../Data/Few_nostop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22052 ../Data/Few_filtered_nostop.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../Data/Few_filtered_nostop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../Data/Few_nostop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip ../Data/Few_filtered_nostop.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if everything is done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( '../Data/Few_filtered_nostop.txt.gz', names=['Tweets', 'Targets'], \n",
    "                   compression='gzip', delimiter='\\t' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaand back twittertimeout funni nazi like wan...</td>\n",
       "      <td>hungary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aadhar link mobil bank mandatori aadhar link v...</td>\n",
       "      <td>election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa japan new kenyan market robust partnership ...</td>\n",
       "      <td>market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aam jan win elect elect</td>\n",
       "      <td>election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaog aaog anyday market realis drill nail cert...</td>\n",
       "      <td>market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets   Targets\n",
       "0  aaaand back twittertimeout funni nazi like wan...   hungary\n",
       "1  aadhar link mobil bank mandatori aadhar link v...  election\n",
       "2  aa japan new kenyan market robust partnership ...    market\n",
       "3                            aam jan win elect elect  election\n",
       "4  aaog aaog anyday market realis drill nail cert...    market"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218640, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
